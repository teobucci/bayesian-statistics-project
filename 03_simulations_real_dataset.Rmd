---
title: "Stochastic Block Model Prior with Ordering Constraints for Gaussian Graphical Models"
author:
    - Alessandro Colombi (Supervisor)^[a.colombi10@campus.unimib.it]
    - Teo Bucci^[teo.bucci@mail.polimi.it]
    - Filippo Cipriani^[filippo.cipriani@mail.polimi.it]
    - Filippo Pagella^[filippo.pagella@mail.polimi.it]
    - Flavia Petruso^[flavia.petruso@mail.polimi.it]
    - Andrea Puricelli^[andrea3.puricelli@mail.polimi.it]
    - Giulio Venturini^[giulio.venturini@mail.polimi.it]
output:
    html_document:
        toc: true
        toc_float: true
        number_sections: true
    pdf_document:
        toc: true
        toc_depth: 3
        number_section: true
date: "2023-01-17"
editor_options:
    chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulations

## Load the necessary packages

```{r, include = FALSE}
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(mvtnorm)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(FGM)))
suppressWarnings(suppressPackageStartupMessages(library(gmp)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
suppressWarnings(suppressPackageStartupMessages(library(logr)))
```

```{r, include = FALSE}
paths = c(
    "src/utility_functions.R",
    "src/bulky_functions.R",
    "src/data_generation.R"
)

for(p in paths){
    path = file.path(p)
    if(file.exists(path)){
        source(path)
    } else {
        cat("File",path,"was not found in directory, please check.")
    }
}
```


## Generate data

```{r}
suono_centered <- load(here::here("dataset","Suono_centered.Rdat"))
suono_normalized <- load(here::here("dataset","Suono_Quantile_Normalized.Rdat"))

n = nrow(suono_cent)
p = ncol(suono_quant_norm)
```

## Set options for the simulation

```{r}
# https://stackoverflow.com/a/57571028/16222204
# Use TRUE/FALSE instead of T/F

options = set_options(sigma_prior_0=0.5,
                      sigma_prior_parameters=list("a"=1,"b"=1,"c"=1,"d"=1),
                      theta_prior_0=1,
                      theta_prior_parameters=list("c"=1,"d"=1),
                      rho0=p, # start with one group
                      weights_a0=rep(1,p-1),
                      weights_d0=rep(1,p-1),
                      alpha_target=0.234,
                      beta_mu=0.25,
                      beta_sig2=1/16,
                      d=3,
                      alpha_add=0.5,
                      adaptation_step=1/(p*1000),
                      update_sigma_prior=TRUE,
                      update_theta_prior=TRUE,
                      update_weights=TRUE,
                      update_partition=TRUE,
                      update_graph=TRUE,
                      perform_shuffle=TRUE)
```


## Running the simulation

Run the simulation

```{r}
res <- Gibbs_sampler(
    data = suono_cent,
    niter = 8000,
    nburn = 2000,
    thin = 1,
    options = options,
    seed = 123456,
    print = TRUE
)


res_norm <- Gibbs_sampler(
    data = suono_quant_norm,
    niter = 8000,
    nburn = 2000,
    thin = 1,
    options = options,
    seed = 123456,
    print = TRUE
)
```


```{r}
# save an object to a file
filename_data = paste("output/realtest_cent", ".rds", sep = "")
saveRDS(res, file = filename_data)

filename_data = paste("output/realtest_norm", ".rds", sep = "")
saveRDS(res_norm, file = filename_data)
```

# Posterior analysis

Restore the object

```{r} 
#filename_data = paste("output/realtest_cent", ".rds", sep = "") ### CHOOSE ANALYSIS
filename_data = paste("output/realtest_norm", ".rds", sep = "")
res = readRDS(file = filename_data)
```

## Partition

Recomputing the partition in other forms and the number of groups

```{r}
r = do.call(rbind, lapply(res$rho, rho_to_r))
z = do.call(rbind, lapply(res$rho, rho_to_z))
#r_true = rho_to_r(res$true_rho)
#z_true = rho_to_z(res$true_rho)
num_clusters = do.call(rbind, lapply(res$rho, length))
num_clusters = as.vector(num_clusters)
```

### Acceptance frequency

```{r}
mean(res$accepted)
```


### Barplot of changepoints

```{r}
# https://stackoverflow.com/questions/62061859/abline-not-fitting-to-barplot
bar_heights = colSums(r)
#cp_true = which(r_true==1)
#color <- ifelse(seq_along(bar_heights) %in% c(cp_true), "red", "gray")

barplot(
    bar_heights,
    names = seq_along(bar_heights),
    border = "NA",
    space = 0,
    yaxt = "n",
    main="Changepoint frequency distribution",
    #col = color,
    cex.names=.6,
    las=2
)

#abline(v=cp_true-0.5, col="red", lwd=2)
legend("topright", legend=c("True"), col=c("red"),
    bty = "n",
    lty = 1,
    cex = 0.6)
```

### Evolution of the number of clusters

```{r}
plot(
    x = 1:length(num_clusters),
    y = num_clusters,
    type = "n",
    xlab = "Iterations",
    ylab = "Number of groups",
    main = "Number of groups - Traceplot"
)
lines(x = 1:length(num_clusters), y = num_clusters)
#abline(h = length(z_to_rho(z_true)),
#       col = "red",
#       lwd = 4)
legend("topleft", legend=c("True"), col=c("red"),
    lty = 1,
    cex = 1)
```

```{r}
#num_clusters_true = length(rho_true)

histogram = hist(
    num_clusters,
    xlab = "Number of groups",
    ylab = "Frequency",
    border = "darkgray",
    main = paste(
        "Number of groups - Frequency\n",
        "Last:",
        tail(num_clusters, n = 1),
        "- Mean:",
        round(mean(num_clusters), 2)
    ),
    xaxt = "n"
)

axis(
    side = 1,
    at = histogram$mids,
    labels = seq(from = floor(histogram$mids[1]),
                 length.out = length(histogram$mids))
    ) 

# legend(
#     "topright",
#     legend = c(paste("Last:", tail(num_clusters, n = 1)),
#                paste("Mean:", round(mean(num_clusters),2)),
#                paste("True:", num_clusters_true)),
#     col=c(NA,NA,NA),
#     bty = "n",
#     lty = 1,
#     cex = 0.8
# )
```


### Compute similarity matrix

```{r}
# compute similarity matrix 
sim_matrix <- salso::psm(z)


heatmap(sim_matrix, Colv = FALSE, Rowv = FALSE)
```

### Retrieving best partition using VI on visited ones (order is guaranteed here)

Here we are satisfied with finding the optimal partition only in the set of those visited, not in all the possible ones. I expect it could work even worse. But at least it guarantees to find an admissible one.
I would say that it is the implementation of formula (13) of the Corradin-Danese paper (https://doi.org/10.1016/j.ijar.2021.12.019).

```{r}
library("Rcpp")
library("RcppArmadillo")
sourceCpp("src/wade.cpp")

# compute VI loss for all visited partitions
dists <- VI_LB(z, psm_mat = sim_matrix)

# select best partition (among the visited ones)
final_partition_VI <- z[which.min(dists),]
unname(table(final_partition_VI))

```

## Graph

### Plot estimated matrices

```{r}
graph <- tail(res$G,n=1)[[1]]
diag(graph)=1
ACutils::ACheatmap(
    graph,
    use_x11_device = F,
    horizontal = F,
    main = "Estimated plinks matrix",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    tail(res$K,n=1)[[1]],
    use_x11_device = F,
    horizontal = F,
    main = "Estimated Precision matrix K",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
```


### Graph visualization
Extract last plinks

```{r}
library(tidygraph)
library(ggraph)
last_G = res$G[[length(res$G)]]
```

Criterion 1 to select the threshold (should not work very well) and assign final graph

```{r}
threshold = 0.5
final_graph <- matrix(0,p,p)
final_graph[which(last_G>threshold)] = 1
```

Criterion 2 to select the threshold

```{r}
bfdr_select = BFDR_selection(last_G, tol = seq(0.1, 1, by = 0.001))
```

Inspect the threshold and assign final graph

```{r}
bfdr_select$best_treshold
final_graph = bfdr_select$best_truncated_graph
```

Select the graph and plot it against the original


```{r}
# read edges and nodes
g2 <- graph.adjacency(final_graph)
edges2 <- get.edgelist(g2)
edges2 <- cbind(edges2,rep("estimated",nrow(edges2)))

edges <- as.data.frame(rbind(edges2))
names(edges) = c("from","to","graph")


nodes = data.frame(vertices=1:p, estim_clust=as.factor(final_partition_VI))
nodes

g = graph_from_data_frame(edges, directed = FALSE, nodes)
lay = create_layout(g, layout = "fr")

# add node names
ggraph(lay) + 
  geom_edge_link(edge_colour = "grey") + 
  geom_node_point(aes(color=estim_clust), size = 4) +
  geom_node_text(aes(label = name), repel=TRUE)


```




