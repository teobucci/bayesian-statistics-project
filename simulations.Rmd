---
title: "Stochastic Block Model Prior with Ordering Constraints for Gaussian Graphical Models"
author:
    - Alessandro Colombi (Supervisor)^[a.colombi10@campus.unimib.it]
    - Teo Bucci^[teo.bucci@mail.polimi.it]
    - Filippo Cipriani^[filippo.cipriani@mail.polimi.it]
    - Filippo Pagella^[filippo.pagella@mail.polimi.it]
    - Flavia Petruso^[flavia.petruso@mail.polimi.it]
    - Andrea Puricelli^[andrea3.puricelli@mail.polimi.it]
    - Giulio Venturini^[giulio.venturini@mail.polimi.it]
output:
    html_document:
        toc: true
        toc_float: true
        number_sections: true
    pdf_document:
        toc: true
        toc_depth: 3
        number_section: true
date: "2023-01-17"
editor_options:
    chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# pdf(file="saving_plot4.pdf")
# dev.off()
```

# Simulations

## Load the necessary packages

```{r, include = FALSE}
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(mvtnorm)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(FGM)))
suppressWarnings(suppressPackageStartupMessages(library(gmp)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
suppressWarnings(suppressPackageStartupMessages(library(logr)))
```

```{r, include = FALSE}
paths = c(
    "src/utility_functions.R",
    "src/bulky_functions.R",
    "src/data_generation.R"
)

for(p in paths){
    path = file.path(p)
    if(file.exists(path)){
        source(path)
    } else {
        cat("File",path,"was not found in directory, please check.")
    }
}
```


## Generate data

```{r}
# Define true clustering
rho_true = c(8,4,8,5)

# Set seed for data generation
seed_data_gen = 22111996

# Define number of observations
n = 500

# Define variance of the Beta
beta_sig2 = 1/16
```

```{r}
z_true = rho_to_z(rho_true)
r_true = z_to_r(z_true)
p = length(z_true)

# Generate data
sim = Generate_BlockDiagonal(n = n, z_true = z_true)
# sim = Generate_Block(
#     n=n,
#     z_true=z_true,
#     p_block_diag = 1,
#     p_block_extra_diag = 0,
#     p_inside_block = 0.95,
#     p_outside_block = 0.1,
#     elem_out = 5,
#     min_eigenval_correction = 3,
#     seed = 1
# )
```

```{r}
ACutils::ACheatmap(
    sim$Graph,
    use_x11_device = F,
    horizontal = F,
    main = "Graph",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    sim$Prec,
    use_x11_device = F,
    horizontal = F,
    main = "Precision matrix K",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    sim$Cov,
    use_x11_device = F,
    horizontal = F,
    main = "Covariance",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
```

```{r}
Prec_true = sim$Prec # precision matrix
Graph_true = sim$Graph # true graph
Graph_true[col(Graph_true)==row(Graph_true)] = 0 # remove self loops

graph_density = sum(sim$Graph) / (p*(p-1))
```

## Set options for the simulation

Grid for the variance of the Beta

```{r}
(grid_beta_sig2 = seq(from=1/16, to=1/4, length.out = 10))
```

```{r}
# https://stackoverflow.com/a/57571028/16222204
# Use TRUE/FALSE instead of T/F

options = set_options(sigma_prior_0=0.5,
                      sigma_prior_parameters=list("a"=1,"b"=1,"c"=1,"d"=1),
                      theta_prior_0=1,
                      theta_prior_parameters=list("c"=1,"d"=1),
                      rho0=p, # start with one group
                      weights_a0=rep(1,p-1),
                      weights_d0=rep(1,p-1),
                      alpha_target=0.234,
                      beta_mu=graph_density,
                      beta_sig2=beta_sig2,
                      d=3,
                      alpha_add=0.5,
                      adaptation_step=1/(p*1000),
                      update_sigma_prior=TRUE,
                      update_theta_prior=TRUE,
                      update_weights=TRUE,
                      update_partition=TRUE,
                      update_graph=TRUE,
                      perform_shuffle=TRUE)
```


## Running the simulation

Create output directory if needed

```{r}
dir.create(file.path("output", "data"), showWarnings = FALSE, recursive = TRUE)
dir.create(file.path("output", "log"), showWarnings = FALSE, recursive = TRUE)
```

```{r}
unique_ID = uuid::UUIDgenerate(use.time = TRUE, n = 1, output = c("string"))
unique_ID = dittodb::hash(unique_ID, n = 8)
cat("This simulation has been assigned ID:", unique_ID)

filename_data = paste("output/data/simulation_", unique_ID, ".rds", sep = "")
filename_log = paste("output/log/simulation_", unique_ID, ".log", sep = "")
```

Run the simulation

```{r}
log_open(file_name = filename_log, show_notes=FALSE, logdir = FALSE)
res <- Gibbs_sampler(
    data = sim$data,
    niter = 10,
    nburn = 2,
    thin = 1,
    options = options,
    seed = 123456,
    print = TRUE
)
log_close()
```

Before saving, also append true data
```{r}
res$true_rho = rho_true
res$true_precision = sim$Prec
res$true_graph = sim$Graph
```

```{r}
# save an object to a file
saveRDS(res, file = filename_data)
```

# Posterior analysis

Restore the object

```{r}
unique_ID = '1bd23dee'
filename_data = paste("output/data/simulation_", unique_ID, ".rds", sep = "")
res = readRDS(file = filename_data)
```

## Partition

Recomputing the partition in other forms and the number of groups

```{r}
r = do.call(rbind, lapply(res$rho, rho_to_r))
z = do.call(rbind, lapply(res$rho, rho_to_z))
r_true = rho_to_r(res$true_rho)
z_true = rho_to_z(res$true_rho)
num_clusters = do.call(rbind, lapply(res$rho, length))
num_clusters = as.vector(num_clusters)
```

### Acceptance frequency

```{r}
mean(res$accepted)
```


### Barplot of changepoints

```{r}
# https://stackoverflow.com/questions/62061859/abline-not-fitting-to-barplot
bar_heights = colSums(r)
cp_true = which(r_true==1)
color <- ifelse(seq_along(bar_heights) %in% c(cp_true), "red", "gray")

barplot(
    bar_heights,
    names = seq_along(bar_heights),
    border = "NA",
    space = 0,
    yaxt = "n",
    main="Changepoint frequency distribution",
    #col = color,
    cex.names=.6,
    las=2
)

abline(v=cp_true-0.5, col="red", lwd=2)
legend("topright", legend=c("True"), col=c("red"),
    bty = "n",
    lty = 1,
    cex = 0.6)
```

### Evolution of the number of clusters

```{r}
plot(
    x = 1:length(num_clusters),
    y = num_clusters,
    type = "n",
    xlab = "Iterations",
    ylab = "Number of groups",
    main = "Number of groups - Traceplot"
)
lines(x = 1:length(num_clusters), y = num_clusters)
abline(h = length(z_to_rho(z_true)),
       col = "red",
       lwd = 4)
legend("topleft", legend=c("True"), col=c("red"),
    lty = 1,
    cex = 1)
```

```{r}
num_clusters_true = length(rho_true)

histogram = hist(
    num_clusters,
    xlab = "Number of groups",
    ylab = "Frequency",
    border = "darkgray",
    main = paste(
        "Number of groups - Frequency\n",
        "Last:",
        tail(num_clusters, n = 1),
        "- Mean:",
        round(mean(num_clusters), 2),
        "- True:",
        num_clusters_true
    ),
    xaxt = "n"
)

axis(
    side = 1,
    at = histogram$mids,
    labels = seq(from = floor(histogram$mids[1]),
                 length.out = length(histogram$mids))
    ) 

abline(v = num_clusters_true+0.25,
       col = "red",
       lwd = 4)

# legend(
#     "topright",
#     legend = c(paste("Last:", tail(num_clusters, n = 1)),
#                paste("Mean:", round(mean(num_clusters),2)),
#                paste("True:", num_clusters_true)),
#     col=c(NA,NA,NA),
#     bty = "n",
#     lty = 1,
#     cex = 0.8
# )
```

### Evolution of the Rand Index

```{r}
# computing rand index for each iteration
rand_index = apply(z, 1, mcclust::arandi, z_true)

# plotting the traceplot of the index
plot(
    x = 1:length(rand_index),
    y = rand_index,
    type = "n",
    xlab = "Iterations",
    ylab = "Rand Index",
    main = paste(
        "Rand Index - Traceplot\n",
        "Last:",
        round(tail(rand_index, n=1), 3),
        "- Mean:",
        round(mean(rand_index), 2)
    )
)
lines(x = 1:length(rand_index), y = rand_index)
abline(h = 1, col = "red", lwd = 4)
```

### Compute similarity matrix

```{r}
# compute similarity matrix 
sim_matrix <- salso::psm(z)

# adding names for the heatmap
rownames(sim_matrix) = 1:length(z_true)
colnames(sim_matrix) = 1:length(z_true)

heatmap(sim_matrix, Colv = FALSE, Rowv = FALSE)
```

### Retrieving best partition using Binder loss e VI (deprecated: order is NOT guaranteed)

```{r, eval=FALSE}
# compute final partition by minimizing the Binder loss or the VI loss functions
binder = mcclust::minbinder(sim_matrix)
VI     = mcclust.ext::minVI(sim_matrix)

# compare partitions just in terms of number of clusters and cluster cardinalities

cat("Binder loss")
unname(table(binder$cl))

cat("Variance of Information")
unname(table(VI$cl))

cat("True")
unname(table(z_true))

cat("Binder loss Rand Index")
mcclust::arandi(binder$cl, z_true, adjust = F)

cat("Variance of Information Rand Index")
mcclust::arandi(VI$cl, z_true, adjust = T)
```

### Retrieving best partition using VI on visited ones (order is guaranteed here)

Here we are satisfied with finding the optimal partition only in the set of those visited, not in all the possible ones. I expect it could work even worse. But at least it guarantees to find an admissible one.
I would say that it is the implementation of formula (13) of the Corradin-Danese paper (https://doi.org/10.1016/j.ijar.2021.12.019).

```{r}
library("Rcpp")
library("RcppArmadillo")
sourceCpp("src/wade.cpp")

# compute VI loss for all visited partitions
dists <- VI_LB(z, psm_mat = sim_matrix)

# select best partition (among the visited ones)
final_partition_VI <- z[which.min(dists),]
unname(table(final_partition_VI))

# compute Rand Index
mcclust::arandi(final_partition_VI,z_true)
```

## Graph

### Plot estimated matrices

```{r}
ACutils::ACheatmap(
    tail(res$G,n=1)[[1]],
    use_x11_device = F,
    horizontal = F,
    main = "Estimated plinks matrix",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    tail(res$K,n=1)[[1]],
    use_x11_device = F,
    horizontal = F,
    main = "Estimated Precision matrix K",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
```

### Evolution of the Kullback–Leibler

```{r}
kl_dist = do.call(rbind, lapply(res$K, function(k) {
    ACutils::KL_dist(res$true_precision, k)
}))

last = round(tail(kl_dist, n=1), 3)
plot(
    x = 1:length(kl_dist),
    y = kl_dist,
    type = "n",
    xlab = "Iterations",
    ylab = "K-L distance",
    main = paste("Kullback–Leibler distance\nLast value:", last)
)
lines(x = 1:length(kl_dist), y = kl_dist)
```

### Graph visualization

```{r}
library(igraph)
plot_graph = function(adjacency_matrix, title){
    # Create the graph object from the adjacency matrix
    g <- graph.adjacency(adjacency_matrix, mode = "undirected")
    # Plot the graph
    plot(g, main=title)
}
```

Extract last plinks

```{r}
last_G = res$G[[length(res$G)]]
```

Criterion 1 to select the threshold (should not work very well) and assign final graph

```{r}
threshold = 0.5
final_graph <- matrix(0,p,p)
final_graph[which(last_G>threshold)] = 1
```

Criterion 2 to select the threshold

```{r}
bfdr_select = BFDR_selection(last_G, tol = seq(0.1, 1, by = 0.001))
```

Inspect the threshold and assign final graph

```{r}
bfdr_select$best_treshold
final_graph = bfdr_select$best_truncated_graph
```

Select the graph and plot it against the original

```{r}
par(mfrow=c(2,1))
plot_graph(final_graph, "Estimated")
plot_graph(res$true_graph, "True")
par(mfrow=c(1,1))
```

Fancy stuff work in progress

```{r}
library(tidygraph)
library(ggraph)
g <- as_tbl_graph(final_graph, directed = F)
# Plot the graph
ggraph(g, layout="stress") + geom_edge_link() + geom_node_point()
```



```{r}
# read edges and nodes
g1 <- graph.adjacency(res$true_graph)
edges1 <- get.edgelist(g1)
edges1 <- cbind(edges1,rep("true",nrow(edges1)))

g2 <- graph.adjacency(final_graph)
edges2 <- get.edgelist(g2)
edges2 <- cbind(edges2,rep("estimated",nrow(edges2)))

edges <- as.data.frame(rbind(edges1,edges2))
names(edges) = c("from","to","graph")


nodes = data.frame(vertices=1:25,true_clust=as.factor(z_true), estim_clust=as.factor(final_partition_VI))
nodes



#n = nrow(nodes)
#m = nrow(edges)

# mutate edges and nodes
#edge_type = sample(c("love", "friendship"), m, replace = TRUE)
#edge_weight = runif(m, 1, 10)
#edges = mutate(edges, type = edge_type, weight = edge_weight)
#nodes = mutate(nodes, id = 1:n) %>% select(id, everything())
#
## degree of nodes (number of ties for each dolphin)
#tb = tibble(v = c(1:n, edges$x, edges$y))
#d = count(tb, v)$n - 1
#nodes = mutate(nodes, degree = d)

# create graph from data frames

g = graph_from_data_frame(edges, directed = FALSE, nodes)

#g <- as_tbl_graph(final_graph, directed = F)
lay = create_layout(g, layout = "fr")

# plot with ggraph
ggraph(lay) + 
  geom_edge_link() + 
  geom_node_point() +
  theme_graph()

# unset graph theme 
# unset_graph_style()

# add node names
ggraph(lay) + 
  geom_edge_link(edge_colour = "grey") + 
  geom_node_point(aes(color=true_clust, shape = estim_clust), size = 4) +
  geom_node_text(aes(label = name), repel=TRUE)

ggraph(lay) + 
  geom_edge_link(aes(color=graph)) + 
  geom_node_point(aes(color=true_clust, shape = estim_clust), size = 4) +
  geom_node_text(aes(label = name), repel=TRUE) +
  facet_edges(~graph)

ggraph(lay) + 
  geom_edge_link(edge_colour = "grey") + 
  geom_node_point(aes(color=true_clust, shape = estim_clust), size = 4) +
  geom_node_text(aes(label = name), repel=TRUE) +
  facet_edges(~graph)


```




