---
title: "Stochastic Block Model Prior with Ordering Constraints for Gaussian Graphical Models"
output:
    html_document:
        toc: true
        toc_float: true
        number_sections: true
    pdf_document:
        toc: true
        toc_depth: 3
        number_section: true
date: "2023-01-17"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulations

## Load the necessary packages

```{r}
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(mvtnorm)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(FGM)))
suppressWarnings(suppressPackageStartupMessages(library(gmp)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
suppressWarnings(suppressPackageStartupMessages(library(logr)))
```

```{r}
paths = c(
    "src/utility_functions.R",
    "src/bulky_functions.R",
    "src/data_generation.R"
)

for(p in paths){
    path = file.path(p)
    if(file.exists(path)){
        source(path)
    } else {
        cat("File",path,"wasn't found in directory, please check.")
    }
}
```


## Generate data

```{r}
# Define true clustering
#z_true = c(1,1,1,1,1,1,1, # cluster 1
#           2,2,2,2,2,2,2, # cluster 2
#           3,3,3,3,3,3,3, # cluster 3
#           4,4,4,4,4,4,4, # cluster 4
#           5,5,5,5,
#           6,6,6,6,6,
#           7,7,7,
#           8,8,8,8,8,8,
#           9,
#           10,
#           11) # cluster 

rho_true = c(7,8,7,4,6)
z_true = rho_to_z(rho_true)
# Define data dimension
p = length(z_true)
n = 500

# z_to_rho(z_true)
# z_to_r(z_true)

# Generate data
sim = Generate_BlockDiagonal(n = n, z_true = z_true)
#sim = Generate_Block(n = n, z_true = z_true)

ACutils::ACheatmap(
    sim$G,
    use_x11_device = F,
    horizontal = F,
    main = "Graph",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
                  
# Plot Cov
ACutils::ACheatmap(
    sim$Cov,
    use_x11_device = F,
    horizontal = F,
    main = "Covariance",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

```

```{r}
Prec_true = sim$Prec # precision matrix
Graph_true = sim$Graph # true graph
Graph_true[col(Graph_true)==row(Graph_true)] = 0 # remove self loops

graph_density = sum(sim$Graph) / (p*(p-1))
```

## Set options for the simulation

```{r}
# https://stackoverflow.com/a/57571028/16222204
# Use TRUE/FALSE instead of T/F

options = set_options(sigma_prior_0=0.5,
                      sigma_prior_parameters=list("a"=1,"b"=1,"c"=1,"d"=1),
                      theta_prior_0=1,
                      theta_prior_parameters=list("c"=1,"d"=1),
                      rho0=p, # start with one group
                      weights_a0=rep(1,p-1),
                      weights_d0=rep(1,p-1),
                      alpha_target=0.234,
                      mu_beta=graph_density,
                      sig2_beta=1/16,
                      d=3,
                      alpha_add=0.5,
                      adaptation_step=1/(p*1000), # as suggested in Benson
                      update_sigma_prior=TRUE,
                      update_theta_prior=TRUE,
                      update_weights=TRUE,
                      update_partition=TRUE,
                      update_graph=TRUE,
                      perform_shuffle=TRUE)
```


## Running the simulation

Create output directory if needed

```{r}
dir.create(file.path('output', 'data'), showWarnings = FALSE, recursive = TRUE)
dir.create(file.path('output', 'log'), showWarnings = FALSE, recursive = TRUE)
```

```{r}
unique_ID = uuid::UUIDgenerate(use.time = TRUE, n = 1, output = c("string"))
unique_ID = dittodb::hash(unique_ID, n = 8)
cat('This simulation has been assigned ID:', unique_ID)

#filename = paste("simulation_",gsub(":", "-", Sys.time()),"_file.txt",sep="")
filename_data = paste("output/data/simulation_", unique_ID, ".rds", sep = "")
filename_log = paste("output/log/simulation_", unique_ID, ".log", sep = "")
```

Run the simulation

```{r}
log_open(file_name = filename_log, show_notes=FALSE, logdir = FALSE)
res <- Gibbs_sampler(
    data = sim$data,
    niter = 1500,
    nburn = 300,
    thin = 2,
    options = options,
    seed = 123456,
    print = TRUE
)
log_close()

# save an object to a file
saveRDS(res, file = filename_data)
```

# Posterior analysis

Restore the object

```{r}
res = readRDS(file = filename_data)
```


## Partition

Recomputing the partition in other forms and the number of groups

```{r}
r = do.call(rbind, lapply(res$rho, rho_to_r))
z = do.call(rbind, lapply(res$rho, rho_to_z))
num_clusters = do.call(rbind, lapply(res$rho, length))
num_clusters = as.vector(num_clusters)
```

### Barplot of changepoints

```{r}
# https://stackoverflow.com/questions/62061859/abline-not-fitting-to-barplot
bar_heights = colSums(r)
bar_heights = bar_heights[-c(length(bar_heights))]
barplot(
    bar_heights,
    names = seq_along(bar_heights),
    border = "NA",
    space = 0,
    yaxt = "n"
)
abline(v=indexes-0.5, col='red', lwd=2)
legend("topright", legend=c("True CPs"),
       col=c("red"), lty=1, cex=0.8)
```

### Evolution of the number of clusters

```{r}
plot(
    x =  1:length(num_clusters),
    y = num_clusters,
    type = "n",
    xlab = 'Iterations',
    ylab = 'Number of groups',
    main = 'Number of groups - Traceplot'
)
lines(x =  1:length(num_clusters), y = num_clusters)

abline(h = length(z_to_rho(z_true)),
       col = 'red',
       lwd = 4)
```

```{r}
hist(
    num_clusters,
    xlab = 'Number of groups',
    ylab = 'Frequency',
    #border = 'NA',
    main = 'Number of groups - Frequency'
)

legend(
    "topright",
    legend = c(paste('Last:', tail(num_clusters, n = 1)),
    paste('MCMC mean:', round(mean(num_clusters),2))),
    col=c(NA,NA),
     bty = "n",
    lty = 1,
    cex = 0.8
)
```

### Evolution of the Rand Index

```{r}
# computing rand index for each iteration
rand_index = apply(z, 1, mcclust::arandi, z_true)

# plotting the traceplot of the index
plot(
    x = 1:length(rand_index),
    y = rand_index,
    type = 'n',
    xlab = 'Iterations',
    ylab = 'Rand Index',
    main = paste('Rand Index - Traceplot - Last value:', last)
)
lines(x =  1:length(rand_index), y = rand_index)
abline(h = 1, col = 'red', lwd = 4)
```

### Compute similarity matrix

```{r}
# compute similarity matrix 
sim_matrix <- salso::psm(z)

# adding names for the heatmap
rownames(sim_matrix) = 1:length(z_true)
colnames(sim_matrix) = 1:length(z_true)

heatmap(sim_matrix, Colv = FALSE, Rowv = FALSE)
```

### Computing Binder loss e VI

TODO: is order respected?

```{r}
# compute final partition by minimizing the Binder loss or the VI loss functions
binder = mcclust::minbinder(sim_matrix)
VI     = mcclust.ext::minVI(sim_matrix)

# compare partitions just in terms of number of clusters and cluster cardinalities

cat('Binder loss')
table(binder$cl)

cat('Variance of Information')
table(VI$cl)

cat('True')
table(z_true)

# compute Rand Index

cat('Binder loss')
mcclust::arandi(binder$cl, z_true, adjust = F)

cat('Variance of Information')
mcclust::arandi(VI$cl, z_true, adjust = T)
```

Here we are satisfied with finding the optimal partition only in the set of those visited, not in all the possible ones. I expect it could work even worse. But at least it guarantees to find an admissible one.
I would say that it is the implementation of formula (13) of the Corradin-Danese paper (https://doi.org/10.1016/j.ijar.2021.12.019).

```{r}
library("Rcpp")
library("RcppArmadillo")
sourceCpp("src/wade.cpp")

# compute VI loss for all visited partitions
dists <- VI_LB(z, psm_mat = sim_matrix)

# select best partition (among the visited ones)
final_partition_VI <- z[which.min(dists),]
table(final_partition_VI)

# compute Rand Index
mcclust::arandi(final_partition_VI,z_true)
```

## Graph

### Evolution of the Kullback–Leibler

```{r}
kl_dist = do.call(rbind, lapply(res$K, function(k) {
    ACutils::KL_dist(Prec_true, k)
}))

last = round(kl_dist[length(kl_dist)], 4)
plot(
    x =  1:length(kl_dist),
    y = kl_dist,
    type = "n",
    xlab = 'Iterations',
    ylab = 'K-L distance',
    main = paste('Kullback–Leibler distance - Last value:', last)
)
lines(x =  1:length(kl_dist), y = kl_dist)
```

### Graph visualization

```{r}
library(igraph)
plot_graph = function(adjacency_matrix, title){
    # Create the graph object from the adjacency matrix
    g <- graph.adjacency(adjacency_matrix, mode = "undirected")
    # Plot the graph
    plot(g, main=title)
}
```

Extract last plinks

```{r}
last_G = res$G[[length(res$G)]]
```

Criterion 1 to select the threshold (shouldn't work very well) and assign final graph

```{r}
threshold = 0.5
final_graph <- matrix(0,p,p)
final_graph[which(last_G>threshold)] = 1
```

Criterion 2 to select the threshold

```{r}
bfdr_select = BFDR_selection(last_G, tol = seq(0.1, 1, by = 0.001))
```

Inspect the threshold and assign final graph

```{r}
bfdr_select$best_treshold
final_graph = bfdr_select$best_truncated_graph
```

Select the graph and plot it against the original

```{r}
par(mfrow=c(2,1))
plot_graph(final_graph, 'Estimated')
plot_graph(Graph_true, 'True')
par(mfrow=c(1,1))
```

Fancy stuff work in progress

```{r}
library(tidygraph)
library(ggraph)
g <- as_tbl_graph(final_graph, directed = F)
# Plot the graph
ggraph(g, layout='stress') + geom_edge_link() + geom_node_point()
```



