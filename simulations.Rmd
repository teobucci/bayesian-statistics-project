---
title: "Stochastic Block Model Prior with Ordering Constraints for Gaussian Graphical Models"
author:
    - Alessandro Colombi (Supervisor)^[a.colombi10@campus.unimib.it]
    - Teo Bucci^[teo.bucci@mail.polimi.it]
    - Filippo Cipriani^[filippo.cipriani@mail.polimi.it]
    - Filippo Pagella^[filippo.pagella@mail.polimi.it]
    - Flavia Petruso^[flavia.petruso@mail.polimi.it]
    - Andrea Puricelli^[andrea3.puricelli@mail.polimi.it]
    - Giulio Venturini^[giulio.venturini@mail.polimi.it]
output:
    html_document:
        toc: true
        toc_float: true
        number_sections: true
    pdf_document:
        toc: true
        toc_depth: 3
        number_section: true
date: "2023-01-17"
editor_options:
    chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# pdf(file="saving_plot4.pdf")
# dev.off()
```

# Simulations

## Load the necessary packages

```{r}
suppressWarnings(suppressPackageStartupMessages(library(tidyverse)))
suppressWarnings(suppressPackageStartupMessages(library(ACutils)))
suppressWarnings(suppressPackageStartupMessages(library(mvtnorm)))
suppressWarnings(suppressPackageStartupMessages(library(salso)))
suppressWarnings(suppressPackageStartupMessages(library(FGM)))
suppressWarnings(suppressPackageStartupMessages(library(gmp)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust)))
suppressWarnings(suppressPackageStartupMessages(library(mcclust.ext)))
suppressWarnings(suppressPackageStartupMessages(library(logr)))
```

```{r}
paths = c(
    "src/utility_functions.R",
    "src/bulky_functions.R",
    "src/data_generation.R"
)

for(p in paths){
    path = file.path(p)
    if(file.exists(path)){
        source(path)
    } else {
        cat("File",path,"was not found in directory, please check.")
    }
}
```


## Generate data

```{r}
# Define true clustering
rho_true = c(7,8,2,3,6)

# Define number of observations
n = 500
```


```{r}
z_true = rho_to_z(rho_true)
r_true = z_to_r(z_true)
p = length(z_true)

# Generate data
#sim = Generate_BlockDiagonal(n = n, z_true = z_true)
sim = Generate_Block(
    n=n,
    z_true=z_true,
    p_block_diag = 0.9,
    p_block_extra_diag = 0.2,
    p_inside_block = 1,
    p_outside_block = 0,
    elem_out = 5,
    min_eigenval_correction = 3,
    seed = 1
)
```

```{r}
ACutils::ACheatmap(
    sim$Graph,
    use_x11_device = F,
    horizontal = F,
    main = "Graph",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    sim$Prec,
    use_x11_device = F,
    horizontal = F,
    main = "Precision matrix K",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    sim$Cov,
    use_x11_device = F,
    horizontal = F,
    main = "Covariance",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
```

```{r}
Prec_true = sim$Prec # precision matrix
Graph_true = sim$Graph # true graph
Graph_true[col(Graph_true)==row(Graph_true)] = 0 # remove self loops

graph_density = sum(sim$Graph) / (p*(p-1))
```

## Set options for the simulation

Grid for the variance of the Beta

```{r}
(grid_beta_sig2 = seq(from=1/16, to=1/4, length.out = 10))
```

```{r}
# https://stackoverflow.com/a/57571028/16222204
# Use TRUE/FALSE instead of T/F

options = set_options(sigma_prior_0=0.5,
                      sigma_prior_parameters=list("a"=1,"b"=1,"c"=1,"d"=1),
                      theta_prior_0=1,
                      theta_prior_parameters=list("c"=1,"d"=1),
                      rho0=p, # start with one group
                      weights_a0=rep(1,p-1),
                      weights_d0=rep(1,p-1),
                      alpha_target=0.234,
                      beta_mu=graph_density,
                      beta_sig2=1/16,
                      d=3,
                      alpha_add=0.5,
                      adaptation_step=1/(p*1000), # as suggested in Benson
                      update_sigma_prior=TRUE,
                      update_theta_prior=TRUE,
                      update_weights=TRUE,
                      update_partition=TRUE,
                      update_graph=TRUE,
                      perform_shuffle=TRUE)
```


## Running the simulation

Create output directory if needed

```{r}
dir.create(file.path("output", "data"), showWarnings = FALSE, recursive = TRUE)
dir.create(file.path("output", "log"), showWarnings = FALSE, recursive = TRUE)
```

```{r}
unique_ID = uuid::UUIDgenerate(use.time = TRUE, n = 1, output = c("string"))
unique_ID = dittodb::hash(unique_ID, n = 8)
cat("This simulation has been assigned ID:", unique_ID)

#filename = paste("simulation_",gsub(":", "-", Sys.time()),"_file.txt",sep="")
filename_data = paste("output/data/simulation_", unique_ID, ".rds", sep = "")
filename_log = paste("output/log/simulation_", unique_ID, ".log", sep = "")
```

Run the simulation

```{r}
log_open(file_name = filename_log, show_notes=FALSE, logdir = FALSE)
start_time <- Sys.time()
res <- Gibbs_sampler(
    data = sim$data,
    niter = 8000,
    nburn = 2000,
    thin = 1,
    options = options,
    seed = 123456,
    print = TRUE
)
cat("Execution time:", Sys.time() - start_time)
log_close()

# save an object to a file
saveRDS(res, file = filename_data)
```

# Posterior analysis

Restore the object

```{r}
res = readRDS(file = filename_data)
```

## Partition

Recomputing the partition in other forms and the number of groups

```{r}
r = do.call(rbind, lapply(res$rho, rho_to_r))
z = do.call(rbind, lapply(res$rho, rho_to_z))
num_clusters = do.call(rbind, lapply(res$rho, length))
num_clusters = as.vector(num_clusters)
```

### Acceptance frequency

```{r}
mean(res$accepted)
```


### Barplot of changepoints

```{r}
# https://stackoverflow.com/questions/62061859/abline-not-fitting-to-barplot
bar_heights = colSums(r)
bar_heights = bar_heights[-c(length(bar_heights))] # remove last
color <- ifelse(seq_along(bar_heights) %in% c(cp_true), "red", "gray")
barplot(
    bar_heights,
    names = seq_along(bar_heights),
    border = "NA",
    space = 0,
    yaxt = "n",
    main="Changepoint frequency distribution",
    col = color
)
# cp_true = which(r_true==1)
# abline(v=cp_true-0.5, col="red", lwd=2)
legend("topright", legend=c("True"), col=c("red"),
    bty = "n",
    lty = 1,
    cex = 0.6)
```

### Evolution of the number of clusters

```{r}
plot(
    x = 1:length(num_clusters),
    y = num_clusters,
    type = "n",
    xlab = "Iterations",
    ylab = "Number of groups",
    main = "Number of groups - Traceplot"
)
lines(x = 1:length(num_clusters), y = num_clusters)
abline(h = length(z_to_rho(z_true)),
       col = "red",
       lwd = 4)
```

```{r}
num_clusters_true = length(rho_true)
color <- ifelse(sort(unique(num_clusters)) %in% c(num_clusters_true), "red", "gray")

hist(
    num_clusters,
    xlab = "Number of groups",
    ylab = "Frequency",
    border = "NA",
    main = "Number of groups - Frequency",
    col = color
)

legend(
    "topright",
    legend = c(paste("Last:", tail(num_clusters, n = 1)),
               paste("Mean:", round(mean(num_clusters),2)),
               paste("True:", num_clusters_true)),
    col=c(NA,NA,NA),
    bty = "n",
    lty = 1,
    cex = 0.8
)
```

### Evolution of the Rand Index

```{r}
# computing rand index for each iteration
rand_index = apply(z, 1, mcclust::arandi, z_true)

last = round(tail(rand_index, n=1), 3)

# plotting the traceplot of the index
plot(
    x = 1:length(rand_index),
    y = rand_index,
    type = "n",
    xlab = "Iterations",
    ylab = "Rand Index",
    main = paste("Rand Index - Traceplot\nLast value:", last)
)
lines(x = 1:length(rand_index), y = rand_index)
abline(h = 1, col = "red", lwd = 4)
```

### Compute similarity matrix

```{r}
# compute similarity matrix 
sim_matrix <- salso::psm(z)

# adding names for the heatmap
rownames(sim_matrix) = 1:length(z_true)
colnames(sim_matrix) = 1:length(z_true)

heatmap(sim_matrix, Colv = FALSE, Rowv = FALSE)
```

### Computing Binder loss e VI

TODO: is order respected?

```{r}
# compute final partition by minimizing the Binder loss or the VI loss functions
binder = mcclust::minbinder(sim_matrix)
VI     = mcclust.ext::minVI(sim_matrix)

# compare partitions just in terms of number of clusters and cluster cardinalities

cat("Binder loss")
unname(table(binder$cl))

cat("Variance of Information")
unname(table(VI$cl))

cat("True")
unname(table(z_true))

cat("Binder loss")
mcclust::arandi(binder$cl, z_true, adjust = F)

cat("Variance of Information")
mcclust::arandi(VI$cl, z_true, adjust = T)
```

Here we are satisfied with finding the optimal partition only in the set of those visited, not in all the possible ones. I expect it could work even worse. But at least it guarantees to find an admissible one.
I would say that it is the implementation of formula (13) of the Corradin-Danese paper (https://doi.org/10.1016/j.ijar.2021.12.019).

```{r}
library("Rcpp")
library("RcppArmadillo")
sourceCpp("src/wade.cpp")

# compute VI loss for all visited partitions
dists <- VI_LB(z, psm_mat = sim_matrix)

# select best partition (among the visited ones)
final_partition_VI <- z[which.min(dists),]
unname(table(final_partition_VI))

# compute Rand Index
mcclust::arandi(final_partition_VI,z_true)
```

## Graph

### Plot estimated matrices

```{r}
ACutils::ACheatmap(
    tail(res$G,n=1)[[1]],
    use_x11_device = F,
    horizontal = F,
    main = "Estimated plinks matrix",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)

ACutils::ACheatmap(
    tail(res$K,n=1)[[1]],
    use_x11_device = F,
    horizontal = F,
    main = "Estimated Precision matrix K",
    center_value = NULL,
    col.upper = "black",
    col.center = "grey50",
    col.lower = "white"
)
```

### Evolution of the Kullback–Leibler

```{r}
kl_dist = do.call(rbind, lapply(res$K, function(k) {
    ACutils::KL_dist(Prec_true, k)
}))

last = round(tail(kl_dist, n=1), 3)
plot(
    x = 1:length(kl_dist),
    y = kl_dist,
    type = "n",
    xlab = "Iterations",
    ylab = "K-L distance",
    main = paste("Kullback–Leibler distance\nLast value:", last)
)
lines(x = 1:length(kl_dist), y = kl_dist)
```

### Graph visualization

```{r}
library(igraph)
plot_graph = function(adjacency_matrix, title){
    # Create the graph object from the adjacency matrix
    g <- graph.adjacency(adjacency_matrix, mode = "undirected")
    # Plot the graph
    plot(g, main=title)
}
```

Extract last plinks

```{r}
last_G = res$G[[length(res$G)]]
```

Criterion 1 to select the threshold (should not work very well) and assign final graph

```{r}
threshold = 0.5
final_graph <- matrix(0,p,p)
final_graph[which(last_G>threshold)] = 1
```

Criterion 2 to select the threshold

```{r}
bfdr_select = BFDR_selection(last_G, tol = seq(0.1, 1, by = 0.001))
```

Inspect the threshold and assign final graph

```{r}
bfdr_select$best_treshold
final_graph = bfdr_select$best_truncated_graph
```

Select the graph and plot it against the original

```{r}
par(mfrow=c(2,1))
plot_graph(final_graph, "Estimated")
plot_graph(Graph_true, "True")
par(mfrow=c(1,1))
```

Fancy stuff work in progress

```{r}
library(tidygraph)
library(ggraph)
g <- as_tbl_graph(final_graph, directed = F)
# Plot the graph
ggraph(g, layout="stress") + geom_edge_link() + geom_node_point()
```



