%!TEX root = ./main.tex

\section{Introduction}
%First paragraph introducing GGM
\gls{ggm} are probabilistic models where undirected graphs are used to express the conditional dependence structure among variables with a joint Gaussian distribution. The vertexes of the graph represent the variables of the model, whereas the edges model the dependency between them. A crucial concept in \gls{ggm} is the one of conditional independence: given a p-dimensional random vector $\mathbf{Y}$ distributed as a multivariate normal with zero mean and precision matrix $\mathbf{K}$, the random variables $Y_i$ and $Y_j$ are independent, conditionally on all the others, if and only if the corresponding entry in precision matrix $\mathbf{K}$ is null. This translates into the absence of an undirected edge linking nodes $i$ and $j$ in the graph. 

The graph is usually unknown and must be learnt from the data. Following a Bayesian approach, the graph itself is considered a random variable, which can assume values in the space of all possible undirected graphs with p nodes. A common choice is to place a uniform prior distribution over such space. Furthermore, a prior on the precision matrix conditionally on the graph must also be introduced. Due to its conjugacy property, a G-Wishart distribution is often selected as a prior for the precision matrix.

%Use SBM as prior to inducing a block structure 
In many real-life applications, the variables of interest can be grouped into clusters. This is a well-known and studied problem in network analysis, where the network of dependencies is observed and one only aim is to study the properties. In such a context, the most widely used model is the \gls{sbm}. More in detail, \gls{sbm} is a generative model for random networks with wide applications in the context of community detection, allowing to cluster the nodes of a graph into mutually exclusive groups that share similar connectivity patterns and are not known a priori. Most importantly, in an \gls{sbm}, the probability of having an edge between two nodes only depends on the group membership of the nodes. Recently, there is a growing literature which aims to extend this reasoning also to problems where the network is latent, not observed. If so, the idea is to exploit \gls{sbm} as a prior for the graph to simultaneously perform structural learning and the clustering of the nodes.

%Additional specification on the order of the partition - IN PROGRESS -
One assumption for clustering often made in the literature is that the nodes' names can be relabeled. This is equivalent to assuming that there is a lack of ordering among the variables. However, we argue that this assumption may fail in some contexts, especially those related to spatial statistics, where the localization of the variables imposes a natural ordering constraint. For instance, several biological problems require the identification of groups of genes which regulate specific cellular functions; in this case, respecting the order of alignment of genomic loci along the chromosome might yield additional important information. Another situation where the variables' order may play a significant role is imaging, where any reasonable grouping of variables should consider the underlying 3-D structure of data.

%Introduce the goal of the project
In this project, we propose a flexible prior that allows us to infer a block-structured graph while respecting an ordering constraint on the nodes. To do so, we build upon the theory of changepoint models from the works of \textcite{bensonAdaptiveMCMCMultiple2018} and \textcite{martinezNonparametricChangePoint2014}, borrowing some ideas concerning the prior on the partition and relying on an adaptive approach. 

%Details on the structure
In the next two sections, we introduce the proposed model and describe the sampling strategy, which relies on a two-block Gibbs sampler to update the graph and the partition. Then, we present the main results of our simulations. Finally, we perform a critical analysis of our results and discuss the limitations of the current work while also identifying potential directions for future investigation.


%THE MODEL---------------------------------------------------------------
\section{Proposed Model}

We consider an independent and identically distributed sample of size $n$ with $p$ variables from a multivariate Normal distribution. Without any loss of generality, we assume that the distribution has zero-mean and covariance matrix $\mathbf{K}^{-1}$ QUESTION: IS IT BETTER TO WRITE DIRECTLY "PRECISION MATRIX K"? As anticipated, each variable corresponds to a node on the underlying graph. Our goal is to simultaneously infer the conditional dependence structure of such variables and their clustering, keeping in mind that the partition of the nodes must respect the original order of the variables. We set a G-Wishart prior distribution for the precision matrix $\mathbf{K}$, and we rely on SBM for the prior on $G$, which is identified by its adjacency matrix.

%SBM
Let $M$ be the number of groups, which is not known a priori, and $\mathbf{z} \in \mathbb{R}^p$ be the vector of group membership so that $z_i \in \left \{1,\ldots,M \right \}$ denotes the group node $i$ belongs to. In SBM, the probability of having an edge from node $i$ to node $j$ only depends on their group membership. The probability that edge $(i,j)$ belongs to the set of edges $E$ is
\begin{align*}
P((i,j)\in E|\mathbf{z},Q)=Q_{z_i,z_j}, \text{independent}
\end{align*}
where $Q$ is a symmetric probability matrix and $Q_{uv}$ is the probability of having an edge between any node in cluster $u$ and any other node in cluster $v$. The standard choice for the prior of $Q$ given the group membership vector is a beta distribution QUESTION: DO WE REMOVE THE FORMULA AND JUST SAY $Q_UV$ IS A BETA WITH PARAMETER A AND B, OR IS IT BETTER TO LEAVE IT LIKE THAT?
\begin{align*}
    Q_{uv} \mid \mathbf{z} & \overset{\mathrm{iid}}{\sim} Beta(a,b),\, 1\leq u \leq v \leq M
\end{align*}
The Beta distribution is conjugate to the prior for $G$, and can be integrated out. After this step, the prior for the graph conditionally on the vector of group membership becomes
\[
    P(G\mid \mathbf{z})
    =
    \prod_{u=1}^{M}\prod_{v=u}^{M}
    \frac{B(\alpha + S_{uv}, \beta+ S^{\star}_{uv})}{B(\alpha,\beta)}
\]
$S^{\star}_{uv}$ is the sum of the ``non-edges'', namely $S^{\star}_{uv} = T_{uv} - S_{uv}$ and $T_{uv}$ is the number of possible edges. \\       

%Sezione changepoint prior
Given this model, our goal is to propose a prior distribution for z which account for the ordering constraint on the nodes, assigning a probability law over the space of admissible partitions. To do so, we introduce an equivalent formulation for the vector of cluster memberships $\mathbf{z}$, introducing the partition vector $\rho_p=\{C_1,\ldots,C_M\}$ representing the partition of the $p$ nodes into $m$ groups, where $C_i$ is the cardinality of group $i$ for $i=1,...,m$. The relationship with $\mathbf{z}$ is given by 
\[
z_i=m \iff i \in C_m, \quad i=1, \ldots, p, \quad m=1, \ldots, M.
\]
Choosing a law for $\mathbf{z}$ is therefore equivalent to specifying a law for $\rho_p$, which will be the focus of the next paragraphs. From this Section onwards, we will switch from one representation to the other accordingly to the context. To introduce an ordering constraint on the law of $\rho_p$, we need to restrict to the space of partition to the admissible ones.
We say that $\rho_p$ is admissible if, for each $i<j$, each element of $C_i$ is strictly smaller than each element of $C_j$. 
As a prior for our model, we use the Exchangeable Partition Probability Function (EPPF)induced by the two-parameter Poisson-Dirichlet process (Pitman-Yor process) from \textcite[830]{martinezNonparametricChangePoint2014}:
\[
    P(\bm{\rho} = (n_1, \ldots, n_M))
    =
    \begin{cases}
        \frac{p!}{M!} \frac{ \prod_{i=1}^{M-1}{(\theta +i\sigma)} }{(\theta+1)_{(p-1)\uparrow}} \prod_{j=1}^{M}{\frac{(1-\sigma)_{(n_{j}-1)\uparrow}}{n_{j\uparrow}} }, & \bm{\rho} \text{ admissible}\\
                0, & \bm{\rho} \text{ not admissible.}
    \end{cases}
\]
where $x_{n\uparrow}$ is the rising factorial (or Pochhammer function), namely
\begin{equation*}
x_{n\uparrow} = \overbrace{x(x+1)(x+2)\cdots(x+n-1)}^{n\text{ factors}} \qquad x_{0\uparrow}=1.
\end{equation*}

$\theta$ and $\sigma$ are hyperparameters such that $\sigma\in[0,1)$ with $\theta>-\sigma$ or $\sigma<0$ with $\theta=m|\sigma|$ for some positive integer $m$. We will work with the case $\sigma\in[0,1)$.\\

Finally, we can rewrite the model as
 \begin{align*}
    \mathbf{y}_1,\ldots,\mathbf{y}_n \mid \mathbf{K} & \overset{\mathrm{iid}}{\sim} \mathcal{N}_p(0,\mathbf{K}) \\
    \mathbf{K} \mid G & \sim G-Wishart(b,D) \\
    G \mid \mathbf{z} & \sim f_G(G\mid\mathbf{z})\\
    \rho_p & \sim \mathcal{L}(\rho_p)
\end{align*}

where $f_G(G\mid\mathbf{z})$ is given by NUMERO EQ and $\sim \mathcal{L}(\rho_p)$ is given by NUMERO EQ


\section{Sampling strategy}
We propose a Block Gibbs sampling strategy divided into two steps: 
\begin{enumerate}
    \item sampling of the graph $\bm{G}$ and the precision matrix $\bm{K}$, conditionally to the partition $\bm{z}$ and the data $\bm{Y}$,
    \item sampling of the partition $\bm{z}$, conditionally to the graph $\bm{G}$, the precision matrix $\bm{K}$ and the data $\bm{Y}$.
\end{enumerate}
We adopt three possible ways to express the partition depending on the aspect is most convenient and we wish to highlight. In the following we will always denote by $M$ the number of groups the nodes have been partitioned into, with $C_{1}, \dotsc, C_{M}$ the groups, with $n_{j} = |C_{j}|$ and with $p$ the number of nodes.
\begin{itemize}
\item $\bm{\rho} =( |C_{1}| ,\dotsc ,|C_{M}|) =( n_{1} ,\dotsc ,n_{M})$ is the vector of groups cardinalities. Since we have a constraint on the ordering of the nodes, it's enough to specify the cardinalities of each group.
\item $\bm{z} =( z_{1} ,\dotsc ,z_{p})$ is a $p$-dimensional vector of memberships, $z_{j} =m$ if node $j$ belongs to group $C_{m}$
\item $\bm{r} =( r_{1} ,\dotsc ,r_{p-1})$ is a $( p-1)$-dimensional vector which tells us if a node is the right end of a group. The last node is, by definition, always the end of a group, at most its own group, thus by convention can be thought of as $1$.
\end{itemize}
For example, when $p=8$ and $M=4$, let us consider the partition
\[
    (C_{1}, C_{2}, C_{3}, C_{4}) = (\{1\},\{2,3,4\},\{5,6\},\{7,8\})
\]
then we have
\begin{equation*}
\bm{\rho} =( 1,3,2,2) \ \ \iff \ \ \bm{z} =( 1,2,2,2,3,3,4,4) \ \ \iff \ \ \bm{r} =( 1,0,0,1,0,1,0)
\end{equation*}

\subsection{Graph sampling}
The conditional distribution used to sample the graph is:
\[
    P(\bm{K},G \mid \bm{Y},\bm{z}) \propto P(\bm{Y} \mid \bm{K})P(\bm{K} \mid {G}) P(G \mid \bm{z})
\]
Given the membership vector $\bm{z}$, a Birth-and-Death approach is used to sample the graph, as suggested in the work of \textcite{mohammadiBayesianStructureLearning2015a}.
The Birth-and-Death algorithm decides at every iteration of the Gibbs sampling whether to add a new edge to the graph (birth) or delete an already existing one (death).
For this purpose, we modified the \texttt{R} and \texttt{C++} package \texttt{BDgraph} to take into account the dependency from membership vector $\bm{z}$, updating the target distribution and the birth and death rates as follows: (TODO : dovremmo aggiungere i numeri alle formule?)
\[
\text{Birth rate} \propto \frac{P(\bm{G}^{+ e}\mid \bm{z})}{P(\bm{G}\mid \bm{z})} = \frac{S_{uv} + \alpha}{S^{\star}_{uv} + \beta}
\quad
\text{Death rate} \propto \frac{P(\bm{G}^{- e}\mid \bm{z})}{P(\bm{G}\mid \bm{z})} \frac{S^{\star}_{uv} + \beta}{S_{uv} + \alpha}
\]

$S_{uv}$ is the number of existing edges between the nodes from cluster $u$ and the nodes from cluster $v$ and $S^{\star}_{uv}$ is the number of all possible edges between the clusters $u$ and $v$, minus the already existing ones. 

$\alpha$ and $\beta$ are the parameters of the Beta distribution of $Q_{uv}\vert \bm{z}$.

\subsection{Random partition sampling}
After the graph update, the random partition is sampled conditionally on the graph $\bm{G}$ using the conditional distribution:
\begin{equation*}
    P(\bm{z} \mid \bm{Y},\bm{K},\bm{G}) \propto P(\bm{Y} \mid \bm{K})P(\bm{K} \mid \bm{G})P(\bm{G} \mid \bm{z})P(\bm{z}) \propto P(\bm{G} \mid \bm{z})P(\bm{z})
\end{equation*}
To sample from $\bm{z}$, an adaptive split and merge was built from scratch using \texttt{R}.

\subsubsection{Split and merge move}

Suppose the current partition at iteration $t$ is $\bm{z}$.
We propose a new candidate partition $\bm{z}'$ and we either accept it or reject it using Metropolis-Hastings and update the partition accordingly for the next iteration $t+1$.

The proposal distribution $Q(\bm{z},\bm{z}')$ exploits the property that the nodes are ordered.
Depending on the move we choose with some probability a group from the current partition to be split into two groups or two groups to be merged into one.
To this extent, we consider the $\bm{r}$ representation of the partition.

Unless we are forced by extreme cases to choose a split move (case with all the nodes belong to the same group, we can only split) or a merge move (all nodes are in their own group, we can only merge) the algorithm works as follows.

\begin{enumerate}
    \item Perform either a split or a merge move, with probability $\alpha_{\text{split}}$ and $1-\alpha_{\text{split}}$, respectively, usually set to $0.5$.
    \item For a split (merge) move, consider all the \num{0} (\num{1}) only in the partition $\bm{r}$, one of which will be drawn as candidate to become a \num{1} (\num{0}), thus splitting a group into two (merging two groups into one).
    The draw of such candidate is made according to two weights vector as explained in \ref{par:proposalratio}.
    \item Accept or reject using Metropolis-Hastings.
    The acceptance probability is the minimum between $1$ and the product of the graph ratio, the partition ratio, and the proposal ratio. Namely
    \begin{equation}
        \alpha_{\text{accept}} = \min
       \bigg\{1,
       \overbrace{
       \underbrace{\frac{P(\bm{G} \mid \bm{z}')}{P(\bm{G} \mid \bm{z})}}_{\substack{\text{graph}\\\text{ratio}}}
       \underbrace{\frac{P(\bm{z}')}{P(\bm{z})}}_{\substack{\text{partition}\\\text{ratio}}}
       }^{\text{target ratio}}
       \underbrace{\frac{Q(\bm{z}',\bm{z})}{Q(\bm{z},\bm{z}')}}_{\substack{\text{proposal}\\\text{ratio}}}
       \bigg\}
       \label{eq:alphaaccept}
    \end{equation}
\end{enumerate}

\paragraph{Proposal distribution}\label{par:proposalratio}

Using \textcite{bensonAdaptiveMCMCMultiple2018} approach we introduce two $( p-1)$-dimensional vectors iteration-dependent
\begin{equation*}
\mathbf{a}^{( t)} =( a_{1}^{( t)} ,\dotsc ,a_{p-1}^{( t)}) \qquad \mathbf{d}^{( t)} =( d_{1}^{( t)} ,\dotsc ,d_{p-1}^{( t)})
\end{equation*}
where
\begin{itemize}
\item $a_{j}^{( t)}$ is the probability that node $j$ is chosen as candidate for splitting a group at iteration $t$
\item $d_{j}^{( t)}$ is the probability that node $j$ is chosen as candidate for merging a group at iteration $t$
\end{itemize}
They are unnormalized discrete densities that are used to choose the node to perform the split or the merge, namely where to add or remove a $1$ from the $\bm{r}$ partition representation.

For example, supposing a splitting move, the probability of drawing node $i$ for the split is proportional to its weight $a_{i}^{(t)}$.

We then introduce:
\begin{align*}
    a^{\star} = \sum\nolimits_{j:r_j=0}{a_{j}^{(t)}} \qquad d^{\star} = \sum\nolimits_{j:r_j=1}{d_{j}^{(t)}}
\end{align*}

The proposal ratio in \eqref{eq:alphaaccept} becomes
\[
    \frac{Q(\bm{z}',\bm{z})}{Q(\bm{z},\bm{z}')}
    =
    \frac{P(\text{choose merge})}{P(\text{choose split})}
    \cdot 
    \frac{P(\text{merge at node $i$})}{P(\text{split at node $i$})}
    =
    \frac{1-\alpha_{\text{split}}}{\alpha_{\text{split}}}
    \cdot
    \frac{\frac{d_{i}^{(t)}}{d^{\star}+d_{i}^{(t)}}}{\frac{a_{i}^{(t)}}{a^{\star}}}
\]
The first term is the ratio of the probabilities of choosing one move over the other. In the second ratio we have at the denominator the probability of choosing exactly the node $i$ for the split, at the numerator the probability of going back after the split  by choosing the same node $i$ for a merge.

\paragraph{Target ratio}
MODIFICATO DA FLAVIA, HO SPOSTATO SOPRA


\begin{equation*}
    \frac{P(\bm{z}')}{P(\bm{z})}
    =
    \frac{P(\bm{\rho}')}{P(\bm{\rho})}
    =
    \frac{1}{M}(\theta+M\sigma)\frac{(1-\sigma)_{(n_{s}'-1)\uparrow}(1-\sigma)_{(n_{s}'+1)\uparrow}}{(1-\sigma)_{(n_{s}-1)\uparrow}}\frac{n_{s}!}{n'_{s}!n'_{s+1}!}
\end{equation*}


As for the graph ratio in \eqref{eq:alphaaccept}, supposing a split move, the likelihood is given by
\begin{align*}
    P(\bm{G}\mid \bm{z}) &= \prod_{l=1}^M \prod_{l=m}^M \frac{B(\alpha+S_{uv},\beta+S^{\star}_{uv})}{B(\alpha,\beta)} \\
    &= \left(\frac{1}{B(\alpha,\beta)}\right)^\frac{M(M+1)}{2}\prod_{l=1}^M \prod_{l=m}^M B(\alpha+S_{uv},\beta+S^{\star}_{uv})
\end{align*}
It's enough to compute it both with the current partition $\bm{z}$ and with the proposed one $\bm{z}'$ and compute the ratio simplifying common factors. For further details about the resulting expression see \ref{sec:graphratio}.

\subsubsection{Adaptive step}

The adaptive step consists of updating the two weights vectors $\bm{a}^{(t)}$ (in case of a split move) and $\bm{d}^{(t)}$ (in case of a merge move) at each iteration $t$ as in \textcite{bensonAdaptiveMCMCMultiple2018} using the following scheme:
    \begin{itemize}
        \item If a split move at node $i$ has been accepted, then update:
        \[
            \log (a_i^{(t+1)})=\log (a_i^{(t)})+\frac{h}{t/p}(\alpha_{\text{split}}-\alpha_{\text{target}}) .
        \]
        \item If a merge move at node $i$ has been accepted, then update:
        \[
            \log (d_i^{(t+1)})=\log (d_i^{(t)})+\frac{h}{t/p}(\alpha_{\text{merge}}-\alpha_{\text{target}}) .
        \]
    \end{itemize}

Where $h>0$ is the initial adaptation, $t/p$ are the iterations $(t)$ per number of nodes $(p)$, $\alpha_{\text{target}}$ is the target Metropolis-Hastings acceptance rate and $\alpha_{\text{merge}} = 1 - \alpha_{\text{split}}$.


\subsubsection{Shuffle move}

After the split and merge step we perform a shuffle move to improve the mixing of the chain.
The shuffle proposes a new partition by moving a certain number of nodes from a group to an adjacent one.
Specifically, if $M>1$:
\begin{enumerate}
    \item choose $j$ uniformly from $\{1, \ldots, M-1\}$, the group to be shuffled with the $(j+1)$-th;
    \item choose $\ell$ uniformly from $\left\{1, \ldots, n_j+n_{j+1}-1\right\}$ the number of nodes to keep in the $j$-th group and set the proposed random partition as
    \[
        \bm{\rho}'=\left(n_1, \ldots, n_{j-1}, \ell, n_j+n_{j+1}-\ell, \ldots, n_M\right)
    \]
    \item accept or reject using Metropolis-Hastings. Since the proposal is a Uniform, the proposal ratio is 1, thus in the acceptance probability we only have the target ratio
    \[
       \alpha_{\text{shuffle}}
       =
       \min
       \bigg\{1,
       \frac{P(\bm{z}'\mid \bm{G})}{P(\bm{z}\mid \bm{G})}
       \bigg\}
       =
       \min
       \bigg\{1,
       \frac{P(\bm{G}\mid \bm{z}')}{P(\bm{G}\mid \bm{z})}
       \frac{P(\bm{z}')}{P(\bm{z})}
       \bigg\}
    \]
    In this case, the number of groups $M$ doesn't change.
    The first ratio can be found in \ref{sec:graphratioshuffle}.
    The second ratio, after simplifying common factors, is
    \[
        \frac{P(\bm{z}')}{P(\bm{z})} = \frac{(1-\sigma)_{(\ell)\uparrow} (1-\sigma)_{(n_{s}+n_{s+1}-\ell)\uparrow}}{(1-\sigma)_{(n_{s}-1)\uparrow} (1-\sigma)_{(n_{s+1}-1)\uparrow}}
        \cdot \frac{n_{s}! n_{s+1}!}{\ell!(n_{s} + n_{s+1} - \ell)!}
    \]
    
\end{enumerate}

\subsection{Updating the hyperparameters}

Update $\theta$ and $\sigma$ according to \textcite[835-836]{martinezNonparametricChangePoint2014}.


\section{Posterior analysis}


\subsection{Posterior Graph}

To select the posterior graph, a common approach is to choose the graph with the highest posterior probability. However, this method can be unreliable due to the large number of possible graphs and low frequency of occurrence of the same graph in a MCMC sampling. Our solution is to estimate the marginal posterior probabilities for each edge inclusion, which is calculated as:
\[
    \hat{p}_{j k}=\frac{\sum_{t=1}^T \mathbbm{1}_{((j, k) \in E_t)} w(\boldsymbol{G}_t)}{\sum_{t=1}^T w(\boldsymbol{G}_t)}
\]
where $\mathbbm{1}_{\left((j, k) \in E_t\right)}$ is the indicator function for the existence of the edge linking node $j$ and node $k$ at iteration $t$ and $w(\boldsymbol{G}_t)$ is the graph weight (\emph{holding time}) at iteration $t$.
Then the graph is selected based on the edges with posterior probabilities greater than a given threshold $s$. 
Two different thresholds can be considered:
\begin{itemize}
    \item $s$ = 0.5, similar to the median probability model proposed by \textcite{Barbieri2004Optimal};
    \item the Bayesian False Discovery rate \parencite[BFDR;][]{Mller2006FDRAB}
    \[
        \operatorname{BFDR}=\frac{\sum_{j<k}\left(1-\hat{p}_{j k}\right) \mathbbm{1}_{\left(\hat{p}_{j k} \geq s\right)}}{\sum_{j<k} \mathbbm{1}_{\left(\hat{p}_{j k} \geq s\right)}}
    \]
where $s$ is selected so that BFDR is below $0.05$.

\end{itemize}

\subsection{Posterior partition}

We can obtain point estimates of the partition by exploiting a decision theoretic approach based on a specific loss function, solving an optimization problem as
\[
    \hat{\bm{\rho}}=\underset{\bm{\rho} \in \mathcal{C}}{\operatorname{argmin}} \mathbb{E}\left[L\left(\tilde{\bm{\rho}}, \bm{\rho}\right) \mid \bm{Y}_1, \ldots, \bm{Y}_n \right]=\underset{\bm{\rho} \in \mathcal{C}}{\operatorname{argmin}} \sum_{\bm{\rho}^* \in \mathcal{C}} L\left(\bm{\rho}^*, \bm{\rho}\right) \underbrace{P\left(\tilde{\bm{\rho}}=\bm{\rho}^* \mid \bm{Y}_1, \ldots, \bm{Y}_n\right)}_{\text{posterior similarity matrix}},
\]
where $\tilde{\bm{\rho}}$ is the \emph{true} partition, $\mathcal{C}$ is the space of all possible partitions, and $L(\cdot, \cdot): \mathcal{C} \times \mathcal{C} \rightarrow \mathbb{R}$ denotes a loss function.

Different choices can be made for the loss function, such as the Binder loss function. Here, we resort to the \gls{vi} loss function \parencite{Meila2007Comparing}.

It is unfeasible to scan the entire space $\mathcal{C}$. We restrict the optimization problem to a sub-optimal solution within the space $\mathcal{C}^T \subseteq \mathcal{C}$ of the orders visited in $T$ steps of the MCMC sampling.


\section{Simulation study}

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lllllllllrrrrl}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{7}{c}{Data} & \multicolumn{6}{c}{Analysis} \\
\cmidrule(l{3pt}r{3pt}){2-8} \cmidrule(l{3pt}r{3pt}){9-14}
\texttt{sim\_id} & $n$ & $p$ & \texttt{data\_gen} & \texttt{seed} & $\bm{\rho}_0$ & \texttt{beta\_sig2} & $\bm{\rho}_{\text{true}}$ & $\bm{\rho}_{\text{est}}$ & \texttt{accept} & VI & RI & KL & \texttt{time}\\
\midrule
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 1: varying seed}}\\
\cellcolor{gray!6}{\hspace{1em}01} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{22111996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21400 mins}\\
\cellcolor{gray!6}{\hspace{1em}02} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{31051999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21800 mins}\\
\cellcolor{gray!6}{\hspace{1em}03} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27051999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\cellcolor{gray!6}{\hspace{1em}04} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{29061999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\cellcolor{gray!6}{\hspace{1em}05} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{12091997} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21600 mins}\\
\cellcolor{gray!6}{\hspace{1em}06} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27091999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21500 mins}\\
\cellcolor{gray!6}{\hspace{1em}07} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 2: varying beta\_sig2}}\\
\cellcolor{gray!6}{\hspace{1em}08} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.062} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.066} & \cellcolor{gray!6}{0.034} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.22400 mins}\\
\cellcolor{gray!6}{\hspace{1em}09} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.078} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.080} & \cellcolor{gray!6}{0.043} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.186} & \cellcolor{gray!6}{1.20700 mins}\\
\cellcolor{gray!6}{\hspace{1em}10} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.093} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.075} & \cellcolor{gray!6}{0.039} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20600 mins}\\
\cellcolor{gray!6}{\hspace{1em}11} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.108} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.079} & \cellcolor{gray!6}{0.049} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20800 mins}\\
\cellcolor{gray!6}{\hspace{1em}12} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.124} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.067} & \cellcolor{gray!6}{0.040} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}13} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.139} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.060} & \cellcolor{gray!6}{0.035} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20200 mins}\\
\cellcolor{gray!6}{\hspace{1em}14} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.154} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.056} & \cellcolor{gray!6}{0.030} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.193} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}15} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.169} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.047} & \cellcolor{gray!6}{0.024} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}16} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.185} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.034} & \cellcolor{gray!6}{0.022} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.189} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}17} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16200 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 3: varying initial partition}}\\
\cellcolor{gray!6}{\hspace{1em}18} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16300 mins}\\
\cellcolor{gray!6}{\hspace{1em}19} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{singletons} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.020} & \cellcolor{gray!6}{0.015} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.189} & \cellcolor{gray!6}{1.24800 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 4: varying group numerosities}}\\
\cellcolor{gray!6}{\hspace{1em}20} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16000 mins}\\
\cellcolor{gray!6}{\hspace{1em}21} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{1,10,2,9,3} & \cellcolor{gray!6}{1,10,2,9,3} & \cellcolor{gray!6}{0.072} & \cellcolor{gray!6}{0.109} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.181} & \cellcolor{gray!6}{1.25500 mins}\\
\cellcolor{gray!6}{\hspace{1em}22} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.02} & \cellcolor{gray!6}{1,3,2,4,2,3,3,4,3} & \cellcolor{gray!6}{18,7} & \cellcolor{gray!6}{0.397} & \cellcolor{gray!6}{1.020} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.126} & \cellcolor{gray!6}{1.10700 mins}\\
\cellcolor{gray!6}{\hspace{1em}23} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{12,13} & \cellcolor{gray!6}{12,13} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.073} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.255} & \cellcolor{gray!6}{1.36700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 5: varying n}}\\
\cellcolor{gray!6}{\hspace{1em}24} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.082} & \cellcolor{gray!6}{0.046} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.17300 mins}\\
\cellcolor{gray!6}{\hspace{1em}25} & \cellcolor{gray!6}{400} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.072} & \cellcolor{gray!6}{0.036} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.221} & \cellcolor{gray!6}{1.15900 mins}\\
\cellcolor{gray!6}{\hspace{1em}26} & \cellcolor{gray!6}{300} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.070} & \cellcolor{gray!6}{0.041} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.336} & \cellcolor{gray!6}{1.16800 mins}\\
\cellcolor{gray!6}{\hspace{1em}27} & \cellcolor{gray!6}{200} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.074} & \cellcolor{gray!6}{0.045} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.532} & \cellcolor{gray!6}{1.15400 mins}\\
\cellcolor{gray!6}{\hspace{1em}28} & \cellcolor{gray!6}{100} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.111} & \cellcolor{gray!6}{0.241} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{1.487} & \cellcolor{gray!6}{1.11800 mins}\\
\cellcolor{gray!6}{\hspace{1em}29} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{20,5} & \cellcolor{gray!6}{0.236} & \cellcolor{gray!6}{0.539} & \cellcolor{gray!6}{0.273} & \cellcolor{gray!6}{4.212} & \cellcolor{gray!6}{1.09000 mins}\\
\cellcolor{gray!6}{\hspace{1em}30} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.280} & \cellcolor{gray!6}{0.241} & \cellcolor{gray!6}{0.000} & \cellcolor{gray!6}{11.854} & \cellcolor{gray!6}{1.07600 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 6: varying p}}\\
\cellcolor{gray!6}{\hspace{1em}31} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{3,2} & \cellcolor{gray!6}{3,2} & \cellcolor{gray!6}{0.543} & \cellcolor{gray!6}{0.752} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.006} & \cellcolor{gray!6}{0.79960 mins}\\
\cellcolor{gray!6}{\hspace{1em}32} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{10} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{10} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5} & \cellcolor{gray!6}{5,5} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{0.115} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.028} & \cellcolor{gray!6}{0.85055 mins}\\
\cellcolor{gray!6}{\hspace{1em}33} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{15} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{15} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5} & \cellcolor{gray!6}{5,5,5} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.087} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.045} & \cellcolor{gray!6}{1.09400 mins}\\
\cellcolor{gray!6}{\hspace{1em}34} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5} & \cellcolor{gray!6}{5,5,5,5} & \cellcolor{gray!6}{0.103} & \cellcolor{gray!6}{0.070} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.118} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}35} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5} & \cellcolor{gray!6}{0.104} & \cellcolor{gray!6}{0.074} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.170} & \cellcolor{gray!6}{1.33600 mins}\\
\cellcolor{gray!6}{\hspace{1em}36} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{30} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{30} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5} & \cellcolor{gray!6}{0.080} & \cellcolor{gray!6}{0.061} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.140} & \cellcolor{gray!6}{1.49600 mins}\\
\cellcolor{gray!6}{\hspace{1em}37} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{35} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{35} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5,5} & \cellcolor{gray!6}{0.066} & \cellcolor{gray!6}{0.149} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.205} & \cellcolor{gray!6}{1.71700 mins}\\
\cellcolor{gray!6}{\hspace{1em}38} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{40} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{40} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{0.061} & \cellcolor{gray!6}{0.384} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.282} & \cellcolor{gray!6}{1.99900 mins}\\
\cellcolor{gray!6}{\hspace{1em}39} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{45} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{45} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,10,10} & \cellcolor{gray!6}{0.071} & \cellcolor{gray!6}{0.761} & \cellcolor{gray!6}{0.756} & \cellcolor{gray!6}{0.396} & \cellcolor{gray!6}{2.45600 mins}\\
\cellcolor{gray!6}{\hspace{1em}40} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{25,5,5,5,5,5} & \cellcolor{gray!6}{0.059} & \cellcolor{gray!6}{0.853} & \cellcolor{gray!6}{0.364} & \cellcolor{gray!6}{0.367} & \cellcolor{gray!6}{3.05700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 7: using noised block structure}}\\
\cellcolor{gray!6}{\hspace{1em}41} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{B} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.013} & \cellcolor{gray!6}{0.104} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.176} & \cellcolor{gray!6}{1.03100 mins}\\
\bottomrule
\end{tabular}}
\end{table}

\section{Conclusion}






\appendix

\section{Mathematical details}

\subsection{Graph ratio split and merge}\label{sec:graphratio}
After simplifying common factors, the resulting expression in the split case is % TODO forse si potrebbe spiegare ancora meglio
\begin{align*}
    \frac{P(\bm{G} \mid \bm{z}^\prime)}{P(\bm{G} \mid \bm{z})}
    & =\left(\frac{1}{B(\alpha, \beta)}\right)^{M+1} &  \\
    & \quad \times \frac{\prod_{l=1}^{S-1} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=1}^{S-1} f_B(C_l, C_S)} & \text{interactions with terms before}\\
    & \quad \times \frac{\prod_{m=S+2}^{M+1} f_B(C_S^\prime, C_m^\prime) f_B(C_{S+1}^\prime, C_m^\prime)}{\prod_{m=S+1}^M f_B(C_S, C_m)} & \text{interactions with terms after}\\
    & \quad \times \frac{f_B(C_S^\prime, C_{S+1}^\prime) f_B(C_S^\prime, C_S^\prime) f_B(C_{S+1}^\prime, C_{S+1}^\prime)}{f_B(C_S, C_S)} & \text{internal interactions}
\end{align*}
where with $C_{j}$ ($C_{j}^\prime$) we denote group $j$ in the current (proposed) partition and
\[
    f_B(C_u, C_v) = B(\alpha+S_{uv},\beta+S^{\star}_{uv})
\]
Moreover, $S$ is the index of the cluster that is being split into two, thus
\[
    \begin{cases}
        C_{m} = C_{m}^\prime & \forall m<S\\
        C_{S} = C_{S}^\prime \cup C_{S+1}^\prime &\\
        C_{m-1} = C_{m}^\prime & \forall m>S+1\\
    \end{cases}
\]

\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
current  & $C_1$ & $\ldots$ & \multicolumn{2}{c|}{$C_S$} & $\ldots$ & $C_M$    \\
\hline
proposed & $C_1^\prime$ & $\ldots$ & $C_S^\prime$       & $C_{S+1}^\prime$      & $\ldots$ & $C_{M+1}^\prime$\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Graph ratio shuffle}\label{sec:graphratioshuffle}

After simplifying common factors, the resulting expression in the split case is
\begin{align*}
    \frac{P(\bm{G}\mid \bm{z}')}{P(\bm{G}\mid \bm{z})}
    &= \frac{\prod_{l=1}^{S-1} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=1}^{S-1} f_B(C_l, C_S) f_B(C_l, C_{S+1})} & \text{interactions with terms before}\\
    & \quad \times \frac{\prod_{l=S+2}^{M} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=S+2}^{M} f_B(C_l, C_S) f_B(C_l, C_{S+1})} & \text{interactions with terms after}\\
    & \quad \times \frac{f_B(C_S^\prime, C_{S+1}^\prime) f_B(C_S^\prime, C_S^\prime) f_B(C_{S+1}^\prime, C_{S+1}^\prime)}{f_B(C_S, C_{S+1}) f_B(C_S, C_S) f_B(C_{S+1}, C_{S+1})} & \text{internal interactions}
\end{align*}




