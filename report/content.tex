%!TEX root = ./main.tex

\section{Introduction}
%First paragraph introducing GGM
\gls{ggm} are probabilistic models where undirected graphs are used to express the conditional dependence structure among variables with a joint Gaussian distribution. The nodes of the graph represent the variables of the model, whereas the edges model the dependency between them. A crucial concept in \gls{ggm} is the one of conditional independence: given a $p$-dimensional random vector $\bm{Y}$ distributed as a multivariate normal with zero mean and precision matrix $\bm{K}$, the random variables $Y_i$ and $Y_j$ are independent, conditionally on all the others, if and only if the corresponding entry in precision matrix $\bm{K}$ is null. This translates into the absence of an undirected edge linking nodes $i$ and $j$ in the graph. 
\[
    Y_i \indep Y_j \mid \bm{Y}_{-(i j)} \iff(i, j) \notin E \iff K_{i j}=0
\]
where $\bm{Y}_{-(i j)}$ is the random vector containing all elements in $\bm{Y}$ except the $i$-th and the $j$-th.

The graph is usually unknown and must be learnt from the data. Following a Bayesian approach, the graph itself is considered a random variable, which can assume values in the space of all possible undirected graphs with $p$ nodes. A common choice is to place a uniform prior distribution over such space. Furthermore, a prior on the precision matrix conditionally on the graph must also be introduced. Due to its conjugacy property, a G-Wishart distribution is often selected as a prior for the precision matrix.

%Use SBM as prior to inducing a block structure 
In many real-life applications, the variables of interest can be grouped into clusters. This is a well-known and studied problem in network analysis, where the network of dependencies is observed and one only aims to study the properties. In such a context, the most widely used model is the \gls{sbm}. More in detail, \gls{sbm} is a generative model for random networks with wide applications in the context of community detection, allowing to cluster the nodes of a graph into mutually exclusive groups, not known a priori, that share similar connectivity patterns. Most importantly, in an \gls{sbm}, the probability of having an edge between two nodes only depends on the group membership of the nodes. Recently, there is a growing literature which aims to extend this reasoning also to problems where the network is latent, not observed. If so, the idea is to exploit \gls{sbm} as a prior for the graph to simultaneously perform structural learning and the clustering of the nodes.

%Additional specification on the order of the partition - IN PROGRESS -
One assumption for clustering often made in the literature is that the nodes' names can be relabeled. This is equivalent to assuming that there is a lack of ordering among the variables. However, we argue that this assumption may fail in some contexts, especially those related to spatial statistics, where the localization of the variables imposes a \textbf{natural ordering constraint}. For instance, several biological problems require the identification of groups of genes which regulate specific cellular functions; in this case, respecting the order of alignment of genomic loci along the chromosome might yield additional important information. Another situation where the variables' order may play a significant role is imaging, where any reasonable grouping of variables should consider the underlying 3-D structure of data.

%Introduce the goal of the project
In this project, we propose a flexible prior that allows us to infer a block-structured graph while respecting an ordering constraint on the nodes. To do so, we build upon the theory of changepoint models from the works of \textcite{bensonAdaptiveMCMCMultiple2018} and \textcite{martinezNonparametricChangePoint2014}, borrowing ideas concerning the prior on the partition and relying on an adaptive approach. 

%Details on the structure
In the next two sections, we introduce the proposed model and describe the sampling strategy, which relies on a two-block Gibbs sampling to update the graph and the partition. Then, we present the main results of our simulations. Finally, we perform a critical analysis and discuss the limitations of the current work while also identifying potential directions for future investigation.


%THE MODEL---------------------------------------------------------------
\section{Proposed Model}

We consider an independent and identically distributed sample of size $n$ with $p$ variables from a multivariate normal distribution. Without any loss of generality, we assume that the distribution has zero-mean and precision matrix $\bm{K}$. As anticipated, each variable corresponds to a node on the underlying graph.

%SBM
As concerns the group memberships, we adopt three equivalent ways to express the partition, depending on the aspect that is most convenient and we wish to highlight. In the following, we will always denote by $M$ the number of groups the nodes have been partitioned into, which is not known a priori, with $C_{1}, \dotsc, C_{M}$ the groups, with $n_{j} = |C_{j}|$ and with $p$ the number of nodes (i.e. number of variables).
\begin{itemize}
    \item $\bm{\rho} =( |C_{1}| ,\dotsc ,|C_{M}|) =( n_{1} ,\dotsc ,n_{M})$ is the vector of groups cardinalities. Since we have a constraint on the ordering of the nodes, it's enough to specify the cardinalities of each group.
    \item $\bm{z} =( z_{1} ,\dotsc ,z_{p})$ is a $p$-dimensional vector of memberships, $z_{j} =m$ if node $j$ belongs to group $C_{m}$
    \item $\bm{r} =( r_{1} ,\dotsc ,r_{p-1})$ is a $( p-1)$-dimensional vector in which $r_j=1$ if node $j$ is the right end of a group, and $0$ otherwise. The last node is, by definition, always the end of a group, thus by convention can be thought of as $1$.
\end{itemize}
For example, when $p=8$ and $M=4$, let us consider the partition
\[
    (C_{1}, C_{2}, C_{3}, C_{4}) = (\{1\},\{2,3,4\},\{5,6\},\{7,8\})
\]
then we have
\begin{equation*}
\bm{\rho} =( 1,3,2,2) \ \ \iff \ \ \bm{z} =( 1,2,2,2,3,3,4,4) \ \ \iff \ \ \bm{r} =( 1,0,0,1,0,1,0)
\end{equation*}

Our goal is to simultaneously infer the conditional dependence structure of such variables and their clustering, keeping in mind that the partition on the nodes must respect the original order of the variables. We place a G-Wishart prior distribution for the precision matrix $\bm{K}$, and we rely on \gls{sbm} for the prior on $\bm{G}$, which is identified by its adjacency matrix.

%SBM
In \gls{sbm}, the probability of having an edge from node $i$ to node $j$ only depends on their group membership. The probability that edge $(i,j)$ belongs to the set of edges $E$ is
\begin{align*}
P((i,j)\in E\mid\bm{z},\bm{Q})=Q_{z_i,z_j} \qquad i,j=1,\ldots,p \quad \text{independent}
\end{align*}
where $\bm{Q}$ is a symmetric probability matrix and $Q_{uv}$ is the probability of having an edge between any node in cluster $u$ and any other node in cluster $v$.
To exploit conjugacy, we place a Beta prior on $\bm{Q}\mid\bm{z}$
\begin{align*}
    Q_{uv} \mid \bm{z} & \overset{\mathrm{iid}}{\sim} \operatorname{Beta}(\alpha,\beta),\quad 1\leq u \leq v \leq M
\end{align*}
The Beta distribution is conjugate to the prior for $\bm{G}$, and can be integrated out. After this step, the prior for the graph conditionally on the vector of group membership reduces to
\begin{equation}
    P(\bm{G}\mid \bm{z})
    =
    \prod_{u=1}^{M}\prod_{v=u}^{M}
    \frac{B(\alpha + S_{uv}, \beta+ S^{\star}_{uv})}{B(\alpha,\beta)}
    \label{eq:graphlikelihood}
\end{equation}
$S_{uv}$ is the number of existing edges between the nodes from cluster $u$ and the nodes from cluster $v$ and $S^{\star}_{uv}$ is the number of all possible edges between the clusters $u$ and $v$, minus the already existing ones. 

%Sezione changepoint prior
Given this model, our goal is to propose a prior distribution for $\bm{z}$ which accounts for the ordering constraint on the nodes, assigning a probability law over the space of admissible partitions.
Choosing a law for $\bm{z}$ is therefore equivalent to specifying a law for $\bm{\rho}$, which will be the focus of the next paragraphs. From this Section onwards, we will switch from one representation to the other accordingly to the context. To introduce an ordering constraint on the law of $\bm{\rho}$, we need to restrict to the space of partition to the admissible ones.
We define $\bm{\rho}$ as admissible if, for each $i<j$, each element of $C_i$ is strictly smaller than each element of $C_j$. 

As a prior for our model, we use the \gls{eppf} induced by the two-parameter Poisson-Dirichlet process (Pitman-Yor process) from \textcite[830]{martinezNonparametricChangePoint2014}:
\begin{equation}
    P(\bm{\rho} = (n_1, \ldots, n_M))
    =
    \begin{cases}
        \frac{p!}{M!} \frac{ \prod_{i=1}^{M-1}{(\theta +i\sigma)} }{(\theta+1)_{(p-1)\uparrow}} \prod_{j=1}^{M}{\frac{(1-\sigma)_{(n_{j}-1)\uparrow}}{n_{j\uparrow}} }, & \bm{\rho} \text{ admissible}\\
                0, & \bm{\rho} \text{ not admissible.}
    \end{cases}
    \label{eq:prior}
\end{equation}
where $x_{n\uparrow}$ is the rising factorial (or Pochhammer function), namely
\begin{equation*}
x_{n\uparrow} = \overbrace{x(x+1)(x+2)\cdots(x+n-1)}^{n\text{ factors}} \qquad x_{0\uparrow}=1.
\end{equation*}
$\theta$ and $\sigma$ are hyperparameters such that $\sigma\in[0,1)$ with $\theta>-\sigma$ or $\sigma<0$ with $\theta=m|\sigma|$ for some positive integer $m$. We will work with the case $\sigma\in[0,1)$.

We can rewrite the model as
 \begin{align*}
    \bm{Y}_1,\ldots,\bm{Y}_n \mid \bm{K} & \overset{\mathrm{iid}}{\sim} \mathcal{N}_p(0,\bm{K}^{-1}) \\
    \bm{K} \mid \bm{G} & \sim \operatorname{G-Wishart}(b,D) \\
    \bm{G} \mid \bm{z} & \sim P(\bm{G}\mid\bm{z})\\
    \bm{\rho} & \sim P(\bm{\rho})
\end{align*}
where $P(\bm{G}\mid\bm{z})$ is given by \eqref{eq:graphlikelihood} and $P(\bm{\rho})$ is given by \eqref{eq:prior}.


\section{Sampling strategy}
We propose a Block Gibbs sampling strategy divided into two steps: 
\begin{enumerate}
    \item sampling of the graph $\bm{G}$ and the precision matrix $\bm{K}$, conditionally to the partition $\bm{z}$ and the data $\bm{Y}$,
    \item sampling of the partition $\bm{z}$, conditionally to the graph $\bm{G}$, the precision matrix $\bm{K}$ and the data $\bm{Y}$.
\end{enumerate}

\subsection{Graph sampling}
The conditional distribution used to sample the graph is:
\[
    P(\bm{K},\bm{G} \mid \bm{Y},\bm{z}) \propto P(\bm{Y} \mid \bm{K})P(\bm{K} \mid \bm{G}) P(\bm{G} \mid \bm{z})
\]
Given the membership vector $\bm{z}$, a Birth-and-Death approach is used to sample the graph, as suggested in the work of \textcite{mohammadiBayesianStructureLearning2015a}.
The Birth-and-Death algorithm decides at every iteration of the Gibbs sampling whether to add a new edge to the graph (birth) or delete an already existing one (death).
For this purpose, we modified the \texttt{R} and \texttt{C++} package \texttt{BDgraph} to take into account the dependency from membership vector $\bm{z}$, updating the target distribution and the birth and death rates as follows:
\[
\text{Birth rate} \propto \frac{P(\bm{G}^{+ e}\mid \bm{z})}{P(\bm{G}\mid \bm{z})} = \frac{S_{uv} + \alpha}{S^{\star}_{uv} + \beta}
\quad
\text{Death rate} \propto \frac{P(\bm{G}^{- e}\mid \bm{z})}{P(\bm{G}\mid \bm{z})} = \frac{S^{\star}_{uv} + \beta}{S_{uv} + \alpha}
\]
where $\alpha$ and $\beta$ are the parameters of the Beta distribution of $Q_{uv}\mid \bm{z}$ and where $\bm{G}^{\pm e}$ denotes graph $\bm{G}$ with the added/removed edge $e$.

\subsection{Random partition sampling}
After the graph update, the random partition is sampled conditionally on the graph $\bm{G}$ using the conditional distribution:
\begin{equation*}
    P(\bm{z} \mid \bm{Y},\bm{K},\bm{G}) \propto P(\bm{Y} \mid \bm{K})P(\bm{K} \mid \bm{G})P(\bm{G} \mid \bm{z})P(\bm{z}) \propto P(\bm{G} \mid \bm{z})P(\bm{z})
\end{equation*}
To sample from $\bm{z}$, an adaptive split and merge was built from scratch using \texttt{R}.

\subsubsection{Split and merge move}

Suppose the current partition at iteration $t$ is $\bm{z}$.
We propose a new candidate partition $\bm{z}'$ and we either accept it or reject it using Metropolis-Hastings and update the partition accordingly for the next iteration $t+1$.

The proposal distribution $Q(\bm{z},\bm{z}')$ exploits the property that the nodes are ordered.
Depending on the move, we choose with some probability a group from the current partition to be split into two groups or two groups to be merged into one.
To this extent, we consider the $\bm{r}$ representation of the partition.

Unless we are forced by extreme cases to choose a split move (\emph{i.e.}, all the nodes belong to the same group) or a merge move (\emph{i.e.}, all the nodes are in their own single-node group), the algorithm works as follows.

\begin{enumerate}
    \item Perform either a split or a merge move, with probability $\alpha_{\text{split}}$ and $1-\alpha_{\text{split}}$, respectively, usually set to $0.5$.
    \item For a split (merge) move, consider all the \num{0} (\num{1}) only in the partition $\bm{r}$, one of which will be drawn as a candidate to become a \num{1} (\num{0}), thus splitting a group into two (merging two groups into one).
    The draw of such a candidate is made according to two weights vectors as explained in \ref{par:proposalratio}.
    \item Accept or reject using Metropolis-Hastings.
    The acceptance probability is the minimum between $1$ and the product of the graph ratio, the partition ratio, and the proposal ratio. Namely
    \begin{equation}
        \alpha_{\text{accept}} = \min
       \bigg\{1,
       \overbrace{
       \underbrace{\frac{P(\bm{G} \mid \bm{z}')}{P(\bm{G} \mid \bm{z})}}_{\substack{\text{graph}\\\text{ratio}}}
       \underbrace{\frac{P(\bm{z}')}{P(\bm{z})}}_{\substack{\text{partition}\\\text{ratio}}}
       }^{\text{target ratio}}
       \underbrace{\frac{Q(\bm{z}',\bm{z})}{Q(\bm{z},\bm{z}')}}_{\substack{\text{proposal}\\\text{ratio}}}
       \bigg\}
       \label{eq:alphaaccept}
    \end{equation}
\end{enumerate}

\paragraph{Proposal distribution}\label{par:proposalratio}

Using \textcite{bensonAdaptiveMCMCMultiple2018} approach we introduce two $( p-1)$-dimensional vectors iteration-dependent
\begin{equation*}
\mathbf{a}^{( t)} =( a_{1}^{( t)} ,\dotsc ,a_{p-1}^{( t)}) \qquad \mathbf{d}^{( t)} =( d_{1}^{( t)} ,\dotsc ,d_{p-1}^{( t)})
\end{equation*}
where
\begin{itemize}
\item $a_{j}^{( t)}$ is the probability that node $j$ is chosen as a candidate for splitting a group at iteration $t$
\item $d_{j}^{( t)}$ is the probability that node $j$ is chosen as a candidate for merging a group at iteration $t$
\end{itemize}
They are unnormalized discrete densities that are used to choose the node to perform the split or the merge, namely where to add or remove a $1$ from the $\bm{r}$ partition representation.

For example, supposing a splitting move, the probability of drawing node $i$ for the split is proportional to its weight $a_{i}^{(t)}$.

We then introduce:
\begin{align*}
    a^{\star} = \sum\nolimits_{j:r_j=0}{a_{j}^{(t)}} \qquad d^{\star} = \sum\nolimits_{j:r_j=1}{d_{j}^{(t)}}
\end{align*}

The proposal ratio in \eqref{eq:alphaaccept} becomes
\[
    \frac{Q(\bm{z}',\bm{z})}{Q(\bm{z},\bm{z}')}
    =
    \frac{P(\text{choose merge})}{P(\text{choose split})}
    \cdot 
    \frac{P(\text{merge at node $i$})}{P(\text{split at node $i$})}
    =
    \frac{1-\alpha_{\text{split}}}{\alpha_{\text{split}}}
    \cdot
    \frac{\frac{d_{i}^{(t)}}{d^{\star}+d_{i}^{(t)}}}{\frac{a_{i}^{(t)}}{a^{\star}}}
\]
The first term is the ratio of the probabilities of choosing one move over the other. In the second ratio, we have at the denominator the probability of choosing exactly the node $i$ for the split, and at the numerator the probability of going back after the split  by choosing the same node $i$ for a merge.

\paragraph{Target ratio}

In the split case, after simplifying common factors, the partition ratio in \eqref{eq:alphaaccept} is suitably expressed in the $\bm{\rho}$ representation:
\begin{equation*}
    \frac{P(\bm{z}')}{P(\bm{z})}
    =
    \frac{1}{M}(\theta+M\sigma)\frac{(1-\sigma)_{(n_{s}'-1)\uparrow}(1-\sigma)_{(n_{s}'+1)\uparrow}}{(1-\sigma)_{(n_{s}-1)\uparrow}}\frac{n_{s}!}{n'_{s}!n'_{s+1}!}
\end{equation*}
As for the graph ratio in \eqref{eq:alphaaccept}, the likelihood is given by
\begin{align*}
    P(\bm{G}\mid \bm{z}) &= \prod_{l=1}^M \prod_{l=m}^M \frac{B(\alpha+S_{uv},\beta+S^{\star}_{uv})}{B(\alpha,\beta)} = \left(\frac{1}{B(\alpha,\beta)}\right)^\frac{M(M+1)}{2}\prod_{l=1}^M \prod_{l=m}^M B(\alpha+S_{uv},\beta+S^{\star}_{uv})
\end{align*}
It's enough to compute it both with the current partition $\bm{z}$ and with the proposed one $\bm{z}'$ and compute the ratio simplifying common factors. For further details about the resulting expression see \ref{sec:graphratio}.

\subsubsection{Adaptive step}

The adaptive step consists of updating the two weights vectors $\bm{a}^{(t)}$ (in case of a split move) and $\bm{d}^{(t)}$ (in case of a merge move) at each iteration $t$ as in \textcite{bensonAdaptiveMCMCMultiple2018} using the following scheme:
    \begin{itemize}
        \item If a split move at node $i$ has been accepted, then update:
        \[
            \log (a_i^{(t+1)})=\log (a_i^{(t)})+\frac{h}{t/p}(\alpha_{\text{split}}-\alpha_{\text{target}}) .
        \]
        \item If a merge move at node $i$ has been accepted, then update:
        \[
            \log (d_i^{(t+1)})=\log (d_i^{(t)})+\frac{h}{t/p}(\alpha_{\text{merge}}-\alpha_{\text{target}}) .
        \]
    \end{itemize}

Where $h>0$ is the initial adaptation, $t/p$ are the iterations $(t)$ per number of nodes $(p)$, $\alpha_{\text{target}}$ is the target Metropolis-Hastings acceptance rate, and $\alpha_{\text{merge}} = 1 - \alpha_{\text{split}}$.


\subsubsection{Shuffle move}

After the split and merge step we perform a shuffle move to improve the mixing of the chain.
The shuffle proposes a new partition by moving a certain number of nodes from a group to an adjacent one.
Specifically, if $M>1$:
\begin{enumerate}
    \item choose $j$ uniformly from $\{1, \ldots, M-1\}$, the group to be shuffled with the $(j+1)$-th;
    \item choose $\ell$ uniformly from $\left\{1, \ldots, n_j+n_{j+1}-1\right\}$ the number of nodes to keep in the $j$-th group and set the proposed random partition as
    \[
        \bm{\rho}'=\left(n_1, \ldots, n_{j-1}, \ell, n_j+n_{j+1}-\ell, \ldots, n_M\right)
    \]
    \item accept or reject using Metropolis-Hastings. Since the proposal is a Uniform, the proposal ratio is 1, thus in the acceptance probability we only have the target ratio
    \begin{equation}
        \alpha_{\text{shuffle}}
        =
        \min
        \bigg\{1,
        \frac{P(\bm{z}'\mid \bm{G})}{P(\bm{z}\mid \bm{G})}
        \bigg\}
        =
        \min
        \bigg\{1,
        \frac{P(\bm{G}\mid \bm{z}')}{P(\bm{G}\mid \bm{z})}
        \frac{P(\bm{z}')}{P(\bm{z})}
        \bigg\}
        \label{eq:alphashuffle}
    \end{equation}
    In this case, the number of groups $M$ doesn't change.
    The first ratio in \eqref{eq:alphashuffle} can be found in \ref{sec:graphratioshuffle}.
    The second ratio in \eqref{eq:alphashuffle}, after simplifying common factors, is
    \[
        \frac{P(\bm{z}')}{P(\bm{z})} = \frac{(1-\sigma)_{(\ell)\uparrow} (1-\sigma)_{(n_{s}+n_{s+1}-\ell)\uparrow}}{(1-\sigma)_{(n_{s}-1)\uparrow} (1-\sigma)_{(n_{s+1}-1)\uparrow}}
        \cdot \frac{n_{s}! n_{s+1}!}{\ell!(n_{s} + n_{s+1} - \ell)!}
    \]
    
\end{enumerate}

\subsection{Updating the hyperparameters}

Update $\theta$ and $\sigma$ in \eqref{eq:prior} according to \textcite[835-836]{martinezNonparametricChangePoint2014}.


\section{Posterior analysis}


\subsection{Posterior graph}\label{sec:posterior-graph}

To select the posterior graph, a common approach is to choose the graph with the highest posterior probability. However, this method can be unreliable due to the large number of possible graphs and low frequency of occurrence of the same graph in a MCMC sampling. Our solution is to estimate the marginal posterior probabilities for each edge inclusion, which is calculated as:
\[
    \hat{p}_{j k}=\frac{\sum_{t=1}^T \mathbbm{1}_{((j, k) \in E_t)} w(\boldsymbol{G}_t)}{\sum_{t=1}^T w(\boldsymbol{G}_t)}
\]
where $\mathbbm{1}_{\left((j, k) \in E_t\right)}$ is the indicator function for the existence of the edge linking node $j$ and node $k$ at iteration $t$ and $w(\boldsymbol{G}_t)$ is the graph weight (\emph{holding time}) at iteration $t$.
Then the adjacency matrix of the graph is selected based on the edges with posterior probabilities greater than a given threshold $s$. 
Two different thresholds can be considered:
\begin{itemize}
    \item $s$ = 0.5, similar to the median probability model proposed by \textcite{Barbieri2004Optimal};
    \item the Bayesian False Discovery rate \parencite[BFDR;][]{Mller2006FDRAB}
    \[
        \operatorname{BFDR}=\frac{\sum_{j<k}\left(1-\hat{p}_{j k}\right) \mathbbm{1}_{\left(\hat{p}_{j k} \geq s\right)}}{\sum_{j<k} \mathbbm{1}_{\left(\hat{p}_{j k} \geq s\right)}}
    \]
where $s$ is selected so that BFDR is below $0.05$.

\end{itemize}

Generally, we preferred the second criterion.

\subsection{Posterior partition}\label{sec:posterior-partition}

We can obtain point estimates of the partition by exploiting a decision theoretic approach based on a specific loss function, solving an optimization problem as
\[
    \hat{\bm{\rho}}=\underset{\bm{\rho} \in \mathcal{C}}{\operatorname{argmin}} \mathbb{E}\left[L\left(\tilde{\bm{\rho}}, \bm{\rho}\right) \mid \bm{Y}_1, \ldots, \bm{Y}_n \right]=\underset{\bm{\rho} \in \mathcal{C}}{\operatorname{argmin}} \sum_{\bm{\rho}^* \in \mathcal{C}} L\left(\bm{\rho}^*, \bm{\rho}\right) \underbrace{P\left(\tilde{\bm{\rho}}=\bm{\rho}^* \mid \bm{Y}_1, \ldots, \bm{Y}_n\right)}_{\text{posterior similarity matrix}},
\]
where $\tilde{\bm{\rho}}$ is the \emph{true} partition, $\mathcal{C}$ is the space of all possible partitions, and $L(\cdot, \cdot): \mathcal{C} \times \mathcal{C} \rightarrow \mathbb{R}$ denotes a loss function.

Different choices can be made for the loss function, such as the Binder loss function. Here, we resort to the \gls{vi} loss function \parencite{Meila2007Comparing}.

It is unfeasible to scan the entire space $\mathcal{C}$. We restrict the optimization problem to a sub-optimal solution within the space $\mathcal{C}^T \subseteq \mathcal{C}$ of the orders visited in $T$ steps of the MCMC sampling.


\subsection{Performance indexes: Kullback-Leibler}\label{sec:kl}

We used the \gls{kl} distance to compare the inferred precision matrix from \texttt{BDgraph} with the generating precision matrix during the simulations with synthetic data. The definition of \gls{kl} distance is as follows.

Suppose that we have two multivariate normal distributions, with means $\mu_0, \mu_1$ and with (non-singular) covariance matrices $\Sigma_0, \Sigma_1$. If the two distributions have the same dimension, $k$, then the relative entropy between the distributions is as follows:
    \[
        D_{\mathrm{KL}}\left(\mathcal{N}_0 \| \mathcal{N}_1\right)=\frac{1}{2}\left(\operatorname{tr}\left(\Sigma_1^{-1} \Sigma_0\right)-k+\left(\mu_1-\mu_0\right)^{\top} \Sigma_1^{-1}\left(\mu_1-\mu_0\right)+\ln \left(\frac{\operatorname{det} \Sigma_1}{\operatorname{det} \Sigma_0}\right)\right)
    \]



\subsection{Performance indexes: Rand index}\label{sec:ri}

We used the \gls{ri} to compare the estimated partition with the generating partition during the simulations with synthetic data. The \gls{ri} is defined as follows.

Given a set of $n$ elements $S=\left\{o_1, \ldots, o_n\right\}$ and two partitions of $S$ to compare, $X=\left\{X_1, \ldots, X_r\right\}$, a partition of $S$ into $r$ subsets, and $Y=\left\{Y_1, \ldots, Y_s\right\}$, a partition of $S$ into $s$ subsets, define the following:
    \begin{itemize}
        \item $a$, the number of pairs of elements in $S$ that are in the same subset in $X$ and in the same subset in $Y$
        \item $b$, the number of pairs of elements in $S$ that are in different subsets in $X$ and in different subsets in $Y$
        \item c, the number of pairs of elements in $S$ that are in the same subset in $X$ and in different subsets in $Y$
        \item $d$, the number of pairs of elements in $S$ that are in different subsets in $X$ and in the same subset in $Y$
    \end{itemize}
    The Rand index, $R$, is:
    \[
    R=\frac{a+b}{a+b+c+d}=\frac{a+b}{\binom{n}{2}}
    \]
    Intuitively, $a+b$ can be considered as the number of agreements between $X$ and $Y$ and $c+d$ as the number of disagreements between $X$ and $Y$.





\newpage
\section{Simulation study}

We ran a total of 41 simulations, varying different hyperparameters, with 10000 iterations, of 2000 discarded as burnin, and $\alpha_{\text{target}}=0.234$
The Beta was reparametrized with mean and variance, with mean set to the graph density.
Simulations were divided into groups, depending on the parameter tuned, and posterior analysis was carried out on each simulation using the same plots and metrics:
 \begin{itemize}
        \item $\bm{\rho}$ estimated as in \ref{sec:posterior-partition}
        \item Mean acceptance rate
        \item \gls{ri} to measure the similarity between partitions as in \ref{sec:ri}
        \item \gls{kl} distance to compare estimated and generating precision matrices as in \ref{sec:kl}
        \item Plots of the posterior adjacency matrix as in \ref{sec:posterior-graph}
        \item Execution time
\end{itemize}




\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lllllllllrrrrl}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{7}{c}{Data} & \multicolumn{6}{c}{Analysis} \\
\cmidrule(l{3pt}r{3pt}){2-8} \cmidrule(l{3pt}r{3pt}){9-14}
\texttt{sim\_id} & $n$ & $p$ & \texttt{data\_gen} & \texttt{seed} & $\bm{\rho}_0$ & \texttt{beta\_sig2} & $\bm{\rho}_{\text{true}}$ & $\bm{\rho}_{\text{est}}$ & \texttt{accept} & VI & RI & KL & \texttt{time}\\
\midrule
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 1: varying seed}}\\
\cellcolor{gray!6}{\hspace{1em}01} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{22111996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21400 mins}\\
\cellcolor{gray!6}{\hspace{1em}02} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{31051999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21800 mins}\\
\cellcolor{gray!6}{\hspace{1em}03} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27051999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\cellcolor{gray!6}{\hspace{1em}04} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{29061999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\cellcolor{gray!6}{\hspace{1em}05} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{12091997} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21600 mins}\\
\cellcolor{gray!6}{\hspace{1em}06} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27091999} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21500 mins}\\
\cellcolor{gray!6}{\hspace{1em}07} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.21700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 2: varying beta\_sig2}}\\
\cellcolor{gray!6}{\hspace{1em}08} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.062} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.066} & \cellcolor{gray!6}{0.034} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.22400 mins}\\
\cellcolor{gray!6}{\hspace{1em}09} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.078} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.080} & \cellcolor{gray!6}{0.043} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.186} & \cellcolor{gray!6}{1.20700 mins}\\
\cellcolor{gray!6}{\hspace{1em}10} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.093} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.075} & \cellcolor{gray!6}{0.039} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20600 mins}\\
\cellcolor{gray!6}{\hspace{1em}11} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.108} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.079} & \cellcolor{gray!6}{0.049} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20800 mins}\\
\cellcolor{gray!6}{\hspace{1em}12} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.124} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.067} & \cellcolor{gray!6}{0.040} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}13} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.139} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.060} & \cellcolor{gray!6}{0.035} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20200 mins}\\
\cellcolor{gray!6}{\hspace{1em}14} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.154} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.056} & \cellcolor{gray!6}{0.030} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.193} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}15} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.169} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.047} & \cellcolor{gray!6}{0.024} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}16} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.185} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.034} & \cellcolor{gray!6}{0.022} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.189} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}17} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16200 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 3: varying initial partition}}\\
\cellcolor{gray!6}{\hspace{1em}18} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16300 mins}\\
\cellcolor{gray!6}{\hspace{1em}19} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{singletons} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.020} & \cellcolor{gray!6}{0.015} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.189} & \cellcolor{gray!6}{1.24800 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 4: varying group numerosities}}\\
\cellcolor{gray!6}{\hspace{1em}20} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16000 mins}\\
\cellcolor{gray!6}{\hspace{1em}21} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{1,10,2,9,3} & \cellcolor{gray!6}{1,10,2,9,3} & \cellcolor{gray!6}{0.072} & \cellcolor{gray!6}{0.109} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.181} & \cellcolor{gray!6}{1.25500 mins}\\
\cellcolor{gray!6}{\hspace{1em}22} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.02} & \cellcolor{gray!6}{1,3,2,4,2,3,3,4,3} & \cellcolor{gray!6}{18,7} & \cellcolor{gray!6}{0.397} & \cellcolor{gray!6}{1.020} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.126} & \cellcolor{gray!6}{1.10700 mins}\\
\cellcolor{gray!6}{\hspace{1em}23} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{12,13} & \cellcolor{gray!6}{12,13} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.073} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.255} & \cellcolor{gray!6}{1.36700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 5: varying $n$}}\\
\cellcolor{gray!6}{\hspace{1em}24} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.082} & \cellcolor{gray!6}{0.046} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.17300 mins}\\
\cellcolor{gray!6}{\hspace{1em}25} & \cellcolor{gray!6}{400} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.072} & \cellcolor{gray!6}{0.036} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.221} & \cellcolor{gray!6}{1.15900 mins}\\
\cellcolor{gray!6}{\hspace{1em}26} & \cellcolor{gray!6}{300} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.070} & \cellcolor{gray!6}{0.041} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.336} & \cellcolor{gray!6}{1.16800 mins}\\
\cellcolor{gray!6}{\hspace{1em}27} & \cellcolor{gray!6}{200} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.074} & \cellcolor{gray!6}{0.045} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.532} & \cellcolor{gray!6}{1.15400 mins}\\
\cellcolor{gray!6}{\hspace{1em}28} & \cellcolor{gray!6}{100} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.111} & \cellcolor{gray!6}{0.241} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{1.487} & \cellcolor{gray!6}{1.11800 mins}\\
\cellcolor{gray!6}{\hspace{1em}29} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{20,5} & \cellcolor{gray!6}{0.236} & \cellcolor{gray!6}{0.539} & \cellcolor{gray!6}{0.273} & \cellcolor{gray!6}{4.212} & \cellcolor{gray!6}{1.09000 mins}\\
\cellcolor{gray!6}{\hspace{1em}30} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.1} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.280} & \cellcolor{gray!6}{0.241} & \cellcolor{gray!6}{0.000} & \cellcolor{gray!6}{11.854} & \cellcolor{gray!6}{1.07600 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 6: varying $p$}}\\
\cellcolor{gray!6}{\hspace{1em}31} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{3,2} & \cellcolor{gray!6}{3,2} & \cellcolor{gray!6}{0.543} & \cellcolor{gray!6}{0.752} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.006} & \cellcolor{gray!6}{0.79960 mins}\\
\cellcolor{gray!6}{\hspace{1em}32} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{10} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{10} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5} & \cellcolor{gray!6}{5,5} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{0.115} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.028} & \cellcolor{gray!6}{0.85055 mins}\\
\cellcolor{gray!6}{\hspace{1em}33} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{15} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{15} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5} & \cellcolor{gray!6}{5,5,5} & \cellcolor{gray!6}{0.129} & \cellcolor{gray!6}{0.087} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.045} & \cellcolor{gray!6}{1.09400 mins}\\
\cellcolor{gray!6}{\hspace{1em}34} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{20} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5} & \cellcolor{gray!6}{5,5,5,5} & \cellcolor{gray!6}{0.103} & \cellcolor{gray!6}{0.070} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.118} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}35} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5} & \cellcolor{gray!6}{0.104} & \cellcolor{gray!6}{0.074} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.170} & \cellcolor{gray!6}{1.33600 mins}\\
\cellcolor{gray!6}{\hspace{1em}36} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{30} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{30} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5} & \cellcolor{gray!6}{0.080} & \cellcolor{gray!6}{0.061} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.140} & \cellcolor{gray!6}{1.49600 mins}\\
\cellcolor{gray!6}{\hspace{1em}37} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{35} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{35} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5,5} & \cellcolor{gray!6}{0.066} & \cellcolor{gray!6}{0.149} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.205} & \cellcolor{gray!6}{1.71700 mins}\\
\cellcolor{gray!6}{\hspace{1em}38} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{40} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{40} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{0.061} & \cellcolor{gray!6}{0.384} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.282} & \cellcolor{gray!6}{1.99900 mins}\\
\cellcolor{gray!6}{\hspace{1em}39} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{45} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{45} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{5,5,5,5,5,10,10} & \cellcolor{gray!6}{0.071} & \cellcolor{gray!6}{0.761} & \cellcolor{gray!6}{0.756} & \cellcolor{gray!6}{0.396} & \cellcolor{gray!6}{2.45600 mins}\\
\cellcolor{gray!6}{\hspace{1em}40} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{50} & \cellcolor{gray!6}{0.0625} & \cellcolor{gray!6}{5,5,5,5,5,5,5,5,5,5} & \cellcolor{gray!6}{25,5,5,5,5,5} & \cellcolor{gray!6}{0.059} & \cellcolor{gray!6}{0.853} & \cellcolor{gray!6}{0.364} & \cellcolor{gray!6}{0.367} & \cellcolor{gray!6}{3.05700 mins}\\
\addlinespace[0.3em]
\multicolumn{14}{l}{\textbf{Group 7: using noised block structure}}\\
\cellcolor{gray!6}{\hspace{1em}41} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{B} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{0.013} & \cellcolor{gray!6}{0.104} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.176} & \cellcolor{gray!6}{1.03100 mins}\\
\bottomrule
\end{tabular}}
\end{table}



\newpage

\subsection{Group 1: changing seed}
A single full posterior analysis is reported below as example. 
For all the following groups, only relevant plots highlighting differences are included.

Results are consistent for every seed, with an estimated adjacency matrix of the graph close to the generating one. The estimated partition coincide with the generating partition, and the \gls{kl} distances converge to 0.187 for all the simulations in the group.

\begin{figure}
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_02/comparison_G_true_est.pdf}
\end{minipage}
\hfill
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_02/comparison_K_true_est.pdf}
\end{minipage}%
\end{figure}

\fg{0.75}{../output/data/simulation_02/graph.pdf}

\fg{1}{../output/data/simulation_02/indexes.pdf}



\subsection{Group 2: changing the variance of the Beta prior}

The results for group 2 are similar to the ones of the first group. Changing the variance of the Beta prior does not yield significant changes in posterior analysis. The only notable difference is that an increase in variance yields a decrease in mean acceptance rate.

\fg{0.7}{../output/kl_dist_comparison_beta_sig2-1.pdf}

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lllllllllrrrrl}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{7}{c}{Data} & \multicolumn{6}{c}{Analysis} \\
\cmidrule(l{3pt}r{3pt}){2-8} \cmidrule(l{3pt}r{3pt}){9-14}
\texttt{sim\_id} & $n$ & $p$ & \texttt{data\_gen} & \texttt{seed} & $\bm{\rho}_0$ & \texttt{beta\_sig2} & $\bm{\rho}_{\text{true}}$ & $\bm{\rho}_{\text{est}}$ & \texttt{accept} & VI & RI & KL & \texttt{time}\\
\midrule
\addlinespace[0.3em]
\cellcolor{gray!6}{\hspace{1em}08} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.062} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.066} & \cellcolor{gray!6}{0.034} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.22400 mins}\\
\cellcolor{gray!6}{\hspace{1em}09} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.078} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.080} & \cellcolor{gray!6}{0.043} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.186} & \cellcolor{gray!6}{1.20700 mins}\\
\cellcolor{gray!6}{\hspace{1em}10} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.093} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.075} & \cellcolor{gray!6}{0.039} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20600 mins}\\
\cellcolor{gray!6}{\hspace{1em}11} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.108} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.079} & \cellcolor{gray!6}{0.049} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20800 mins}\\
\cellcolor{gray!6}{\hspace{1em}12} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.124} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.067} & \cellcolor{gray!6}{0.040} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}13} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.139} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.060} & \cellcolor{gray!6}{0.035} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.188} & \cellcolor{gray!6}{1.20200 mins}\\
\cellcolor{gray!6}{\hspace{1em}14} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.154} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.056} & \cellcolor{gray!6}{0.030} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.193} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}15} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.169} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.047} & \cellcolor{gray!6}{0.024} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.191} & \cellcolor{gray!6}{1.20100 mins}\\
\cellcolor{gray!6}{\hspace{1em}16} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.185} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.034} & \cellcolor{gray!6}{0.022} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.189} & \cellcolor{gray!6}{1.20000 mins}\\
\cellcolor{gray!6}{\hspace{1em}17} & \cellcolor{gray!6}{500} & \cellcolor{gray!6}{25} & \cellcolor{gray!6}{BD} & \cellcolor{gray!6}{27121996} & \cellcolor{gray!6}{25} & \cellcolor{red!50}{0.2} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{gray!6}{8,4,8,5} & \cellcolor{green!50}{0.019} & \cellcolor{gray!6}{0.017} & \cellcolor{gray!6}{1.000} & \cellcolor{gray!6}{0.187} & \cellcolor{gray!6}{1.16200 mins}\\
\bottomrule
\end{tabular}}
\end{table}





\subsection{Group 3: varying initial partition}
Starting from different initial partitions (i.e. one single partition or all singletons) provide results consistent with the simulation shown in group 1.

\newpage

\subsection{Group 4: varying generating partition's cluster numerosities}
Changing the generating partition's cluster numerosities does not generally influence the result. The only exception comes with small and numerous clusters, as in simulation 22.

The graph is reconstructed accurately. The partition estimate, however, is far from the generating one. 
It should be noted that this situation is highly unrepresentative of real data that would be meaningful to be studied with this kind of model.
Moreover, most of the clusters are indeed recognized as shown in the barplot below: the problem lies in the criteria for selecting the clusters.

\begin{figure}
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_22/comparison_G_true_est.pdf}
\end{minipage}
\hfill
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_22/comparison_K_true_est.pdf}
\end{minipage}%
\end{figure}


\fg{0.75}{../output/data/simulation_22/graph.pdf}

\fg{1}{../output/data/simulation_22/indexes.pdf}


\newpage
\subsection{Group 5: varying number of observations}
As $n$ diminishes and becomes comparable to $p$, the \gls{kl} distance plateaus on progressively higher values, and \gls{vi} and \gls{ri} behave accordingly.
The acceptance rates rises in \textit{more difficult} situations.

\fg{0.7}{../output/kl_dist_comparison_n-1.pdf}



\subsection{Group 6: varying number of nodes}
Increasing $p$ yields consequences comparable to diminishing $n$. 
\fg{0.7}{../output/kl_dist_comparison_p-1.pdf}

\newpage
\subsection{Group 7: using noised block structure as data generator}
The data generator for this simulation is a noised block structure, namely with a small probability of having edges between groups and that some edges inside groups are missing.
The results are good, since with few exceptions the correct block structure is retrieved.

\begin{figure}[H]
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_41/comparison_G_true_est.pdf}
\end{minipage}
\hfill
\begin{minipage}[c]{0.49\linewidth}
\fg{1.05}{../output/data/simulation_41/comparison_K_true_est.pdf}
\end{minipage}%
\end{figure}


\fg{0.75}{../output/data/simulation_41/graph.pdf}



\section{Conclusions and further developments}
In this work, we have introduced a flexible prior to infer a block-structured graph under an ordering constraint on the nodes, borrowing concepts from the theory of changepoint models and using an adaptive approach. To carry out posterior inference, we have fully implemented a Block Gibbs sampling strategy which allowed us to simultaneously perform structural learning of the graph and clustering of the nodes. From the numerical simulations ran to test and evaluate the model, we obtained that the graph selection based on the BFDR led to a precise identification of the underlying graph structure and the variable clustering, as confirmed by the values of the indexes used to assess the model performance (\emph{i.e.}, \gls{vi}, RI, \gls{kl}).

Due to the nature of the work and the limitation of the project timeline to one semester, several challenges remain open for further investigation. One possible direction for future development is, for example, finer hyperparameter tuning. Indeed, for several aspects of the model (\emph{e.g.}, the hyperparameter of the prior for $\theta$ and $\sigma$ in the partition prior), we relied on hyperparameters validated in the literature. Even if the selected ones yielded results coherent with the nature of the problem, more structured approaches such as hyperpriors or Bayesian optimization could be introduced to tackle the problem. Furthermore, another crucial step will entail evaluating the model on existing data.
Finally, it should be worth investigating two facts related to speed performance: we noticed that overall the mean acceptance rates was under the target one, we suspect that it might be related to a fast convergence in the first 2000 iterations discarded as burnin. The other aspect is related to the computational complexity of the execution, which we didn't study in detail, yet using the data from the simulations we were able to guess that it seems to scale with $p^2$, a linear regression can be found in \ref{sec:computational}.

\appendix

\section{Mathematical details}

\subsection{Graph ratio split and merge}\label{sec:graphratio}
After simplifying common factors, the resulting expression in the split case is % TODO forse si potrebbe spiegare ancora meglio
\begin{align*}
    \frac{P(\bm{G} \mid \bm{z}^\prime)}{P(\bm{G} \mid \bm{z})}
    & =\left(\frac{1}{B(\alpha, \beta)}\right)^{M+1} &  \\
    & \quad \times \frac{\prod_{l=1}^{S-1} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=1}^{S-1} f_B(C_l, C_S)} & \text{interactions with terms before}\\
    & \quad \times \frac{\prod_{m=S+2}^{M+1} f_B(C_S^\prime, C_m^\prime) f_B(C_{S+1}^\prime, C_m^\prime)}{\prod_{m=S+1}^M f_B(C_S, C_m)} & \text{interactions with terms after}\\
    & \quad \times \frac{f_B(C_S^\prime, C_{S+1}^\prime) f_B(C_S^\prime, C_S^\prime) f_B(C_{S+1}^\prime, C_{S+1}^\prime)}{f_B(C_S, C_S)} & \text{internal interactions}
\end{align*}
where with $C_{j}$ ($C_{j}^\prime$) we denote group $j$ in the current (proposed) partition and
\[
    f_B(C_u, C_v) = B(\alpha+S_{uv},\beta+S^{\star}_{uv})
\]
Moreover, $S$ is the index of the cluster that is being split into two, thus
\[
    \begin{cases}
        C_{m} = C_{m}^\prime & \forall m<S\\
        C_{S} = C_{S}^\prime \cup C_{S+1}^\prime &\\
        C_{m-1} = C_{m}^\prime & \forall m>S+1\\
    \end{cases}
\]

\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c|c|c|c}
\hline
current  & $C_1$ & $\ldots$ & \multicolumn{2}{c|}{$C_S$} & $\ldots$ & $C_M$    \\
\hline
proposed & $C_1^\prime$ & $\ldots$ & $C_S^\prime$       & $C_{S+1}^\prime$      & $\ldots$ & $C_{M+1}^\prime$\\
\hline
\end{tabular}
\end{table}

\subsection{Graph ratio shuffle}\label{sec:graphratioshuffle}

After simplifying common factors, the resulting expression in the split case is
\begin{align*}
    \frac{P(\bm{G}\mid \bm{z}')}{P(\bm{G}\mid \bm{z})}
    &= \frac{\prod_{l=1}^{S-1} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=1}^{S-1} f_B(C_l, C_S) f_B(C_l, C_{S+1})} & \text{interactions with terms before}\\
    & \quad \times \frac{\prod_{l=S+2}^{M} f_B(C_l^\prime, C_S^\prime) f_B(C_l^\prime, C_{S+1}^\prime)}{\prod_{l=S+2}^{M} f_B(C_l, C_S) f_B(C_l, C_{S+1})} & \text{interactions with terms after}\\
    & \quad \times \frac{f_B(C_S^\prime, C_{S+1}^\prime) f_B(C_S^\prime, C_S^\prime) f_B(C_{S+1}^\prime, C_{S+1}^\prime)}{f_B(C_S, C_{S+1}) f_B(C_S, C_S) f_B(C_{S+1}, C_{S+1})} & \text{internal interactions}
\end{align*}
where with $C_{j}$ ($C_{j}^\prime$) we denote group $j$ in the current (proposed) partition and
\[
    f_B(C_u, C_v) = B(\alpha+S_{uv},\beta+S^{\star}_{uv})
\]
Moreover, $S$ is the index of the cluster that is being shuffled with the $(S+1)$-th, thus
\[
    \begin{cases}
        C_{m} = C_{m}^\prime & \forall m<S\\
        C_{S}\cup C_{S+1}  = C_{S}^\prime \cup C_{S+1}^\prime &\\
        C_{m} = C_{m}^\prime & \forall m>S+1\\
    \end{cases}
\]

\begin{table}[H]
\centering
\begin{tabular}{l|c|c|ccc|c|l}
\hline
current  & $C_1$        & $\ldots$ & \multicolumn{2}{c|}{$C_S$}     & $C_{S+1}$               & $\ldots$ & $C_M$    \\
\hline
proposed & $C_1^\prime$ & $\ldots$ & \hspace*{0.5cm}$C_S^\prime$\hspace*{0.5cm} & \multicolumn{2}{|l|}{$C_{S+1}^\prime$}      & $\ldots$ & $C_{M}^\prime$\\
\hline
\end{tabular}
\end{table}

\section{Computational complexity}\label{sec:computational}

\fg{0.7}{../output/execution_time.pdf}
